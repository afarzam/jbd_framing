{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87cba802",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.chdir(\"/mnt/home/amir/framingdecomp/framingDecomp\")\n",
    "\n",
    "# --- Setup: Environment, Logging, and Config ---\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import yaml\n",
    "import logging\n",
    "import json\n",
    "from utils.model_utils import load_model, load_model_multiGPU\n",
    "\n",
    "# Set working directory to project root if needed\n",
    "os.chdir(\"/mnt/home/amir/framingdecomp/framingDecomp\")\n",
    "\n",
    "# -- uncomment if not using multiple GPUs\n",
    "# device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Add current directory to sys.path for imports\n",
    "sys.path.append('.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c90d40f",
   "metadata": {},
   "source": [
    "# Goal-based split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c69a3c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup: Environment, Logging, and Config ---\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import yaml\n",
    "import logging\n",
    "import json\n",
    "from utils.model_utils import load_model, load_model_multiGPU\n",
    "\n",
    "# Set working directory to project root if needed\n",
    "os.chdir(\"/mnt/home/amir/framingdecomp/framingDecomp\")\n",
    "\n",
    "# -- uncomment if not using multiple GPUs\n",
    "# device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Add current directory to sys.path for imports\n",
    "sys.path.append('.')\n",
    "\n",
    "\n",
    "# Load configuration\n",
    "with open('configs/decomposer3.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(config['experiment']['seed'])\n",
    "np.random.seed(config['experiment']['seed'])\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return [json.loads(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "def split_data_by_framing(data, test_framing_index, train_framing_indices):\n",
    "    \"\"\"\n",
    "    Split data into train and test sets based on framing indices.\n",
    "    Returns (train_data, test_data)\n",
    "    \"\"\"\n",
    "    test_data = [entry for entry in data if entry.get('framing_index') == test_framing_index]\n",
    "    train_data = [entry for entry in data if entry.get('framing_index') in train_framing_indices]\n",
    "    return train_data, test_data\n",
    "\n",
    "# Load data\n",
    "DATA_PATH_varyF = config['data']['input_path_varyFraming']\n",
    "DATA_PATH_varyG = config['data']['input_path_varyGoal']\n",
    "DATA_PATH_varyF_benign = config['data']['input_path_varyFraming_benign']\n",
    "DATA_PATH_varyG_benign = config['data']['input_path_varyGoal_benign']\n",
    "raw_data_varyF = load_jsonl(DATA_PATH_varyF)\n",
    "raw_data_varyG = load_jsonl(DATA_PATH_varyG)\n",
    "raw_data_varyF_benign = load_jsonl(DATA_PATH_varyF_benign)\n",
    "raw_data_varyG_benign = load_jsonl(DATA_PATH_varyG_benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7abc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mark benign / jailbreak flags\n",
    "# benign   = raw_data_varyF_benign + raw_data_varyG_benign\n",
    "# jailbrks = raw_data_varyF + raw_data_varyG\n",
    "# print(f\"{len(benign)=}   {len(jailbrks)=}\")\n",
    "\n",
    "\n",
    "\n",
    "# # --- Splitting Mode: \"random\", \"goal\", or \"category\" ---\n",
    "# split_mode = \"goal\"  # Change to \"random\" or \"category\" as needed\n",
    "# split_ratio = 0.8    # 80% train/ID, 20% test/OOD\n",
    "\n",
    "# def get_split_sets(items, key, split_ratio=0.8):\n",
    "#     \"\"\"Generic function to split by a key (goal_index or Category).\"\"\"\n",
    "#     all_keys = sorted({e[key] for e in items})\n",
    "#     np.random.shuffle(all_keys)\n",
    "#     split_pt = int(split_ratio * len(all_keys))\n",
    "#     ID_KEYS = set(all_keys[:split_pt])\n",
    "#     OOD_KEYS = set(all_keys[split_pt:])\n",
    "#     return ID_KEYS, OOD_KEYS\n",
    "\n",
    "# def mask(items, key_set, key):\n",
    "#     return [e for e in items if e[key] in key_set]\n",
    "\n",
    "# if split_mode == \"goal\":\n",
    "#     np.random.seed(0)\n",
    "#     ID_KEYS, OOD_KEYS = get_split_sets(benign, \"goal_index\", split_ratio)\n",
    "#     mask_key = \"goal_index\"\n",
    "#     id_name = \"id_goal\"\n",
    "#     ood_name = \"ood_goal\"\n",
    "# elif split_mode == \"category\":\n",
    "#     np.random.seed(0)\n",
    "#     ID_KEYS, OOD_KEYS = get_split_sets(benign, \"Category\", split_ratio)\n",
    "#     mask_key = \"Category\"\n",
    "#     id_name = \"id_category\"\n",
    "#     ood_name = \"ood_category\"\n",
    "# elif split_mode == \"random\":\n",
    "#     np.random.seed(0)\n",
    "#     # For random, shuffle and split the actual items, not by key\n",
    "#     def random_split(items, split_ratio=0.8):\n",
    "#         idxs = np.arange(len(items))\n",
    "#         np.random.shuffle(idxs)\n",
    "#         split_pt = int(split_ratio * len(items))\n",
    "#         return [items[i] for i in idxs[:split_pt]], [items[i] for i in idxs[split_pt:]]\n",
    "#     # Split each set\n",
    "#     ben_ID_varyF, ben_OOD_varyF = random_split(raw_data_varyF_benign, split_ratio)\n",
    "#     ben_ID_varyG, ben_OOD_varyG = random_split(raw_data_varyG_benign, split_ratio)\n",
    "#     jb_ID_varyF, jb_OOD_varyF = random_split(raw_data_varyF, split_ratio)\n",
    "#     jb_ID_varyG, jb_OOD_varyG = random_split(raw_data_varyG, split_ratio)\n",
    "#     id_name = \"train\"\n",
    "#     ood_name = \"test\"\n",
    "# else:\n",
    "#     raise ValueError(\"Unknown split_mode\")\n",
    "\n",
    "# if split_mode in [\"goal\", \"category\"]:\n",
    "#     ben_ID_varyF   = mask(raw_data_varyF_benign,   ID_KEYS, mask_key)\n",
    "#     ben_ID_varyG   = mask(raw_data_varyG_benign,   ID_KEYS, mask_key)\n",
    "#     ben_OOD_varyF  = mask(raw_data_varyF_benign,   OOD_KEYS, mask_key)\n",
    "#     ben_OOD_varyG  = mask(raw_data_varyG_benign,   OOD_KEYS, mask_key)\n",
    "#     jb_ID_varyF    = mask(raw_data_varyF,          ID_KEYS, mask_key)\n",
    "#     jb_ID_varyG    = mask(raw_data_varyG,          ID_KEYS, mask_key)\n",
    "#     jb_OOD_varyF   = mask(raw_data_varyF,          OOD_KEYS, mask_key)\n",
    "#     jb_OOD_varyG   = mask(raw_data_varyG,          OOD_KEYS, mask_key)\n",
    "\n",
    "# # --- Output paths ---\n",
    "# def make_out_path(base_path, split_type, id_or_ood):\n",
    "#     folder = split_type + \"/\" + id_or_ood\n",
    "#     return '/'.join(base_path.split('/')[:-1]) + f'/{folder}/' + base_path.split('/')[-1]\n",
    "\n",
    "# out_path_varyF_benign_id  = make_out_path(DATA_PATH_varyF_benign, id_name, \"varyF_benign\")\n",
    "# out_path_varyG_benign_id  = make_out_path(DATA_PATH_varyG_benign, id_name, \"varyG_benign\")\n",
    "# out_path_varyF_benign_ood = make_out_path(DATA_PATH_varyF_benign, ood_name, \"varyF_benign\")\n",
    "# out_path_varyG_benign_ood = make_out_path(DATA_PATH_varyG_benign, ood_name, \"varyG_benign\")\n",
    "# out_path_varyF_id         = make_out_path(DATA_PATH_varyF, id_name, \"varyF\")\n",
    "# out_path_varyG_id         = make_out_path(DATA_PATH_varyG, id_name, \"varyG\")\n",
    "# out_path_varyF_ood        = make_out_path(DATA_PATH_varyF, ood_name, \"varyF\")\n",
    "# out_path_varyG_ood        = make_out_path(DATA_PATH_varyG, ood_name, \"varyG\")\n",
    "\n",
    "# ids_and_oods = [ben_ID_varyF, ben_ID_varyG, \n",
    "#                 ben_OOD_varyF, ben_OOD_varyG,\n",
    "#                 jb_ID_varyF, jb_ID_varyG, \n",
    "#                 jb_OOD_varyF, jb_OOD_varyG]\n",
    "# out_paths = [out_path_varyF_benign_id, out_path_varyG_benign_id,\n",
    "#              out_path_varyF_benign_ood, out_path_varyG_benign_ood,\n",
    "#              out_path_varyF_id, out_path_varyG_id,\n",
    "#              out_path_varyF_ood, out_path_varyG_ood]\n",
    "\n",
    "# # --- Write splits ---\n",
    "# for idx, out_path in enumerate(out_paths):\n",
    "#     os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "#     with open(out_path, \"w\") as f:\n",
    "#         f.write(f\"# model_name: gpt4.1\\n\")\n",
    "#     for e in ids_and_oods[idx]:\n",
    "#         with open(out_path, \"a\") as fout:\n",
    "#             fout.write(json.dumps(e) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52f0cc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(benign)=3234   len(jailbrks)=3429\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mark benign / jailbreak flags\n",
    "benign   = raw_data_varyF_benign + raw_data_varyG_benign\n",
    "jailbrks = raw_data_varyF + raw_data_varyG\n",
    "print(f\"{len(benign)=}   {len(jailbrks)=}\")\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "all_goals = sorted({e[\"goal_index\"] for e in benign})\n",
    "np.random.shuffle(all_goals)\n",
    "split_pt  = int(0.8*len(all_goals))       # 80 % ID, 20 % OOD\n",
    "ID_GOALS  = set(all_goals[:split_pt])\n",
    "OOD_GOALS = set(all_goals[split_pt:])\n",
    "\n",
    "def mask(items, goal_set):  # returns list[str]\n",
    "    return [e for e in items if e[\"goal_index\"] in goal_set]\n",
    "\n",
    "ben_ID_varyF   = mask(raw_data_varyF_benign,   ID_GOALS)\n",
    "ben_ID_varyG   = mask(raw_data_varyG_benign,   ID_GOALS)\n",
    "ben_OOD_varyF  = mask(raw_data_varyF_benign,   OOD_GOALS)\n",
    "ben_OOD_varyG  = mask(raw_data_varyG_benign,   OOD_GOALS)\n",
    "jb_ID_varyF   = mask(raw_data_varyF,   ID_GOALS)\n",
    "jb_ID_varyG   = mask(raw_data_varyG,   ID_GOALS)\n",
    "jb_OOD_varyF  = mask(raw_data_varyF,   OOD_GOALS)\n",
    "jb_OOD_varyG  = mask(raw_data_varyG,   OOD_GOALS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb6905c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./id/data'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_path_varyF_benign_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce22de61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/populated_benign_JBB-behaviors/PAIR'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/'.join(DATA_PATH_varyF_benign.split('/')[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82cc1ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_path_varyF_benign_id = '/'.join(DATA_PATH_varyF_benign.split('/')[:-1]) + '/id/' + DATA_PATH_varyF_benign.split('/')[-1]\n",
    "out_path_varyG_benign_id = '/'.join(DATA_PATH_varyG_benign.split('/')[:-1]) + '/id/' + DATA_PATH_varyG_benign.split('/')[-1]\n",
    "out_path_varyF_benign_ood = '/'.join(DATA_PATH_varyF_benign.split('/')[:-1]) + '/ood/' + DATA_PATH_varyF_benign.split('/')[-1]\n",
    "out_path_varyG_benign_ood = '/'.join(DATA_PATH_varyG_benign.split('/')[:-1]) + '/ood/' + DATA_PATH_varyG_benign.split('/')[-1]\n",
    "out_path_varyF_id = '/'.join(DATA_PATH_varyF.split('/')[:-1]) + '/id/' + DATA_PATH_varyF.split('/')[-1]\n",
    "out_path_varyG_id = '/'.join(DATA_PATH_varyG.split('/')[:-1]) + '/id/' + DATA_PATH_varyG.split('/')[-1]\n",
    "out_path_varyF_ood = '/'.join(DATA_PATH_varyF.split('/')[:-1]) + '/ood/' + DATA_PATH_varyF.split('/')[-1]\n",
    "out_path_varyG_ood = '/'.join(DATA_PATH_varyG.split('/')[:-1]) + '/ood/' + DATA_PATH_varyG.split('/')[-1]\n",
    "\n",
    "ids_and_oods = [ben_ID_varyF, ben_ID_varyG, \n",
    "                ben_OOD_varyF, ben_OOD_varyG,\n",
    "                jb_ID_varyF, jb_ID_varyG, \n",
    "                jb_OOD_varyF, jb_OOD_varyG]\n",
    "out_paths = [out_path_varyF_benign_id, out_path_varyG_benign_id,\n",
    "             out_path_varyF_benign_ood, out_path_varyG_benign_ood,\n",
    "             out_path_varyF_id, out_path_varyG_id,\n",
    "             out_path_varyF_ood, out_path_varyG_ood]\n",
    "for idx, out_path in enumerate(out_paths):\n",
    "    # Write header with model and input data info\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(f\"# model_name: gpt4.1\\n\")\n",
    "    \n",
    "    for e in ids_and_oods[idx]:\n",
    "        with open(out_path, \"a\") as fout:\n",
    "            fout.write(json.dumps(e) + \"\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65edc7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "874d0b25",
   "metadata": {},
   "source": [
    "# Split with 3 Modes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed8a11",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8831fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "#  split_data.py   ·  three-way splitter for framing-decomp datasets\n",
    "# ---------------------------------------------------------------------------\n",
    "#\n",
    "#  USAGE inside a notebook:\n",
    "#  -------------------------------------------------\n",
    "#  %run split_data.py               # uses default (split by goal_index)\n",
    "#  %run split_data.py random        # 80/20 random train / test\n",
    "#  %run split_data.py category      # 80/20 ID_category / OOD_category\n",
    "#\n",
    "#  The script reproduces EXACTLY the old behaviour when\n",
    "#  called with the default “goal” mode.\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "import os, sys, json, yaml, logging, random, argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ─────────────────────────  CONFIG / ARGS  ──────────────────────────\n",
    "# parser = argparse.ArgumentParser(description=\"Dataset splitter (goal | category | random)\")\n",
    "# parser.add_argument(\"mode\", nargs=\"?\", default=\"goal\",\n",
    "#                     choices=[\"goal\", \"category\", \"random\"],\n",
    "#                     help=\"Splitting strategy to use\")\n",
    "# args = parser.parse_args()\n",
    "# SPLIT_MODE = args.mode                        # \"goal\" | \"category\" | \"random\"\n",
    "\n",
    "SPLIT_MODE = \"random\" # choices=[\"goal\", \"category\", \"random\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7948a494",
   "metadata": {},
   "source": [
    "### Load and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cff34710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config (unchanged)\n",
    "with open(\"configs/decomposer3.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "SEED = config[\"experiment\"][\"seed\"]\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ──────────────────────────  helpers  ───────────────────────────────\n",
    "def load_jsonl(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return [json.loads(line.strip()) for line in f if line.strip() and not line.strip().startswith('#')]\n",
    "\n",
    "def write_jsonl(path: str, data: List[dict], header: str | None = None):\n",
    "    os.makedirs(Path(path).parent, exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        if header is not None:\n",
    "            f.write(header + \"\\n\")\n",
    "        for e in data:\n",
    "            f.write(json.dumps(e) + \"\\n\")\n",
    "\n",
    "def inject_dir(original_path: str, tag: str) -> str:\n",
    "    \"\"\"Insert /{tag}/ right before the filename.\"\"\"\n",
    "    parts                = original_path.split(\"/\")\n",
    "    new_dir              = \"/\".join(parts[:-1] + [tag])\n",
    "    os.makedirs(new_dir, exist_ok=True)\n",
    "    return f\"{new_dir}/{parts[-1]}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fecb66",
   "metadata": {},
   "source": [
    "### Split funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2153fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_split(items: List[dict], ratio: float = 0.8) -> Tuple[List[dict], List[dict]]:\n",
    "    idx = np.arange(len(items))\n",
    "    np.random.shuffle(idx)\n",
    "    split = int(ratio * len(items))\n",
    "    return [items[i] for i in idx[:split]], [items[i] for i in idx[split:]]\n",
    "\n",
    "def grouped_split(items: List[dict], key: str, ratio: float = 0.8\n",
    "                  ) -> Tuple[List[dict], List[dict]]:\n",
    "    \"\"\"\n",
    "    Generic 80/20 split by a *group* key (`goal_index` or `category`).\n",
    "    All elements sharing the same key value go to the same side.\n",
    "    \"\"\"\n",
    "    groups = sorted({e[key] for e in items})\n",
    "    rng    = np.random.RandomState(SEED)\n",
    "    rng.shuffle(groups)\n",
    "    split  = int(ratio * len(groups))\n",
    "    in_set = set(groups[:split])           # ID groups\n",
    "    return ([e for e in items if e[key] in in_set],    # ID\n",
    "            [e for e in items if e[key] not in in_set])# OOD\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4fa31a",
   "metadata": {},
   "source": [
    "### run split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d0b7737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_path_varyFraming': './data/populated_artifacts/PAIR/id/all_populated_prompts_gpt4.1_paraphrases10_maxattempts5_noParaphrase.jsonl',\n",
       " 'input_path_varyGoal': './data/populated_artifacts/PAIR/id/all_cleaned_populated_prompts_gpt4.1_goals10_maxattempts5_noParaphrase.jsonl',\n",
       " 'input_path_varyFraming_benign': './data/populated_benign_JBB-behaviors/PAIR/id/populated_prompts_gpt4.1_paraphrases10_maxattempts5_noParaphrase.jsonl',\n",
       " 'input_path_varyGoal_benign': './data/populated_benign_JBB-behaviors/PAIR/id/cleaned_populated_benign_prompts_gpt4.1_goals10_maxattempts5_noParaphrase.jsonl',\n",
       " 'test_framing_index': 0,\n",
       " 'train_framing_indices': [1, 2, 3, 4, 5]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d43286a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'varyF_benign': './data/populated_benign_JBB-behaviors/PAIR/populated_prompts_gpt4.1_paraphrases10_maxattempts5_noParaphrase.jsonl',\n",
       " 'varyG_benign': './data/populated_benign_JBB-behaviors/PAIR/cleaned_populated_benign_prompts_gpt4.1_goals10_maxattempts5_noParaphrase.jsonl',\n",
       " 'varyF': './data/populated_artifacts/PAIR/all_populated_prompts_gpt4.1_paraphrases10_maxattempts5_noParaphrase.jsonl',\n",
       " 'varyG': './data/populated_artifacts/PAIR/all_cleaned_populated_prompts_gpt4.1_goals10_maxattempts5_noParaphrase.jsonl'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fe5b9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RANDOM  ] varyF_benign    ➜   1575 → ./data/populated_benign_JBB-behaviors/PAIR/train/populated_prompts_gpt4.1_paraphrases10_maxattempts5_noParaphrase.jsonl\n",
      "                  varyF_benign    ➜    394 → ./data/populated_benign_JBB-behaviors/PAIR/test/populated_prompts_gpt4.1_paraphrases10_maxattempts5_noParaphrase.jsonl\n",
      "[RANDOM  ] varyG_benign    ➜   1012 → ./data/populated_benign_JBB-behaviors/PAIR/train/cleaned_populated_benign_prompts_gpt4.1_goals10_maxattempts5_noParaphrase.jsonl\n",
      "                  varyG_benign    ➜    253 → ./data/populated_benign_JBB-behaviors/PAIR/test/cleaned_populated_benign_prompts_gpt4.1_goals10_maxattempts5_noParaphrase.jsonl\n",
      "[RANDOM  ] varyF           ➜   1653 → ./data/populated_artifacts/PAIR/train/all_populated_prompts_gpt4.1_paraphrases10_maxattempts5_noParaphrase.jsonl\n",
      "                  varyF           ➜    414 → ./data/populated_artifacts/PAIR/test/all_populated_prompts_gpt4.1_paraphrases10_maxattempts5_noParaphrase.jsonl\n",
      "[RANDOM  ] varyG           ➜   1089 → ./data/populated_artifacts/PAIR/train/all_cleaned_populated_prompts_gpt4.1_goals10_maxattempts5_noParaphrase.jsonl\n",
      "                  varyG           ➜    273 → ./data/populated_artifacts/PAIR/test/all_cleaned_populated_prompts_gpt4.1_goals10_maxattempts5_noParaphrase.jsonl\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────  load data  ─────────────────────────────\n",
    "DP = config[\"data\"]   # just a shorthand\n",
    "\n",
    "datasets = {\n",
    "    #  name                  path\n",
    "    \"varyF_benign\": DP[\"input_path_varyFraming_benign\"],\n",
    "    \"varyG_benign\": DP[\"input_path_varyGoal_benign\"],\n",
    "    \"varyF\"       : DP[\"input_path_varyFraming\"],\n",
    "    \"varyG\"       : DP[\"input_path_varyGoal\"],\n",
    "}\n",
    "\n",
    "# ──────────────────────────  perform split  ────────────────────────\n",
    "split_tags = {     # destination sub-directories for each mode\n",
    "    \"goal\"     : (\"id\", \"ood\"),\n",
    "    \"category\" : (\"id_category\", \"ood_category\"),\n",
    "    \"random_id\"   : (\"train\", \"test\"),\n",
    "    \"random\"   : (\"train\", \"test\"),\n",
    "}\n",
    "\n",
    "\n",
    "if SPLIT_MODE=='random':\n",
    "    for k, v in datasets.items():\n",
    "        if \"/id/\" in v:\n",
    "            datasets[k] = v.replace(\"/id/\", \"/\")\n",
    "        if \"/ood/\" in v:\n",
    "            datasets[k] = v.replace(\"/ood/\", \"/\")\n",
    "\n",
    "loaded = {k: load_jsonl(p) for k, p in datasets.items()}\n",
    "    \n",
    "\n",
    "tag_A, tag_B = split_tags[SPLIT_MODE]\n",
    "header_line  = \"# model_name: gpt4.1\"\n",
    "\n",
    "for name, data in loaded.items():\n",
    "    if SPLIT_MODE == \"goal\":\n",
    "        part_A, part_B = grouped_split(data, key=\"goal_index\")\n",
    "    elif SPLIT_MODE == \"category\":\n",
    "        part_A, part_B = grouped_split(data, key=\"category\")\n",
    "    else:  # random\n",
    "        part_A, part_B = random_split(data)\n",
    "\n",
    "    # build output paths and write\n",
    "    in_path    = datasets[name]\n",
    "    out_A_path = inject_dir(in_path, tag_A)\n",
    "    out_B_path = inject_dir(in_path, tag_B)\n",
    "\n",
    "    write_jsonl(out_A_path, part_A, header_line)\n",
    "    write_jsonl(out_B_path, part_B, header_line)\n",
    "\n",
    "    print(f\"[{SPLIT_MODE.upper():8}] {name:15} ➜  {len(part_A):5} → {out_A_path}\")\n",
    "    print(f\"                  {name:15} ➜  {len(part_B):5} → {out_B_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8537378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6269"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1575 +  + 1012 + 253 + 1653 + 414 + 1089 + 273"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec21f8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "808"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "394 + 414"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84dae5b",
   "metadata": {},
   "source": [
    "### sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41d0fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────  sanity check for “goal” mode  ─────────\n",
    "if SPLIT_MODE == \"goal\":\n",
    "    # regenerate the *exact* ID/OOD sets as in the original script\n",
    "    old_benign   = loaded[\"varyF_benign\"] + loaded[\"varyG_benign\"]\n",
    "    old_goals    = sorted({e[\"goal_index\"] for e in old_benign})\n",
    "    rng          = np.random.RandomState(SEED)\n",
    "    rng.shuffle(old_goals)\n",
    "    cut          = int(0.8 * len(old_goals))\n",
    "    old_ID_goals = set(old_goals[:cut])\n",
    "\n",
    "    new_ID_goals = {e[\"goal_index\"] for e in load_jsonl(\n",
    "        inject_dir(datasets[\"varyF_benign\"], \"id\"))}\n",
    "\n",
    "    assert new_ID_goals == old_ID_goals, (\n",
    "        \"⚠️  Mismatch with the original splitting logic!\")\n",
    "    else_msg = \"✅  Goal-based split matches original output.\"\n",
    "    print(else_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95016b66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Venv",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
