{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e41a15ab",
   "metadata": {},
   "source": [
    "# Jailbreak Detect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e890a919",
   "metadata": {},
   "source": [
    "## Session setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c717f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.chdir(\"/mnt/home/amir/framingdecomp/framingDecomp\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60c86a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure there are multiple gpus available\n",
    "import torch, os\n",
    "print(\"Devices visible:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "print(\"torch.cud:a.device_count():\", torch.cuda.device_count())\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "414edfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "006d9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell: [Session setup] ====\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import yaml\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from accelerate import notebook_launcher\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import chi2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.misc import set_seed\n",
    "from utils.model_utils import load_model\n",
    "from models.encoder import HFEncoder_notPooled\n",
    "from models.decomposer import NonlinearDecomposer, NonlinearDecomposer_tiny\n",
    "\n",
    "# ——— Configuration switches ———\n",
    "USE_MULTIGPU       = False\n",
    "VISIBLE_DEVICES    = \"6\"#\"0,1,2,3,4,5,6,7\"\n",
    "MIXED_PRECISION    = \"fp16\"\n",
    "DETECT_VIA_FRAMING = True\n",
    "\n",
    "# ——— GPU setup ———\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "if USE_MULTIGPU:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = VISIBLE_DEVICES\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}, GPUs available: {torch.cuda.device_count()}\")\n",
    "\n",
    "# ——— Paths & IDs ———\n",
    "CFG_PATH    = \"configs/jb_detect.yaml\"\n",
    "# DECOMP_CKPT   = \"checkpoints/decomposer_plus_finetuned_sae/decomposer_20250623_184312_697330e3-b6ca-42c7-bc54-e055c0660939\"  # <— fill\n",
    "# ENC_LLM_NAME  = \"google/gemma-2-2b\"                                 # same as before\n",
    "# UNIQUE_ID = \"20250627_202604_0fcaf7b2-f040-4a74-baff-4fe40dcfbf74\"  # <— fill\n",
    "# UNIQUE_ID = \"20250701_201331_d2aa58b9-38e4-499a-a95b-dd20be22cfc5\" # llama 2-7b\n",
    "# UNIQUE_ID = \"20250702_160029_5193c5c3-b998-4fe7-93b6-f3ba130cc1fd\" # llama 2-7b\n",
    "UNIQUE_ID = \"20250702_202119_2eee6866-4447-47ee-b196-da0570fe26e5\" # first ID - llama 3-8b\n",
    "UNIQUE_ID = \"20250714_222459_166e19ad-9601-4465-9f1b-057cfd442bd3\"\n",
    "UNIQUE_ID = \"20250714_232018_4e80eac4-6d82-47a0-897e-7f6e668105f6\"\n",
    "UNIQUE_ID = \"20250715_174848_1a6da921-2bd4-494b-90d6-936a90c81a58\"\n",
    "# UNIQUE_ID = \"20250715_184431_6ffd5609-abf9-45d3-933a-14c23a1ae300\"\n",
    "# UNIQUE_ID = \"20250715_222742_8c06b6b8-54ab-4a20-84d9-fb401c983a78\"\n",
    "UNIQUE_ID = \"20250715_234602_01b3a579-63cb-470c-a970-ebc1811b31eb\"\n",
    "UNIQUE_ID = \"20250716_004913_c5d27944-36c1-4b8f-b4ef-db2b7b7ae02c\"\n",
    "UNIQUE_ID = \"20250716_184334_ffbcf508-59a9-4791-bd07-b8d2966f967e\"\n",
    "UNIQUE_ID = \"20250716_195752_94c9045e-7b3c-4b1d-b8e2-1ff9d3aed158\"\n",
    "# UNIQUE_ID = \"20250717_083408_fafa7c5a-4fe0-420d-b08f-1bef4cf3ea5f\"\n",
    "UNIQUE_ID = \"20250717_101812_06190f25-e1f1-4ed4-87ae-a51365b6061b\"\n",
    "UNIQUE_ID = \"20250717_185022_ffe5f4d3-277b-4ea3-89fa-bf1ec76322e9\"\n",
    "UNIQUE_ID = \"20250719_232328_fc7e001b-2263-4f0d-a7e3-7a614ea80326\" # all layers llama 2\n",
    "\n",
    "# UNIQUE_ID = \"20250715_154742_e25aed12-7215-42ac-a06f-012306e4cdb9\"\n",
    "# UNIQUE_ID = \"20250702_211211_91f6b1c3-f0a6-48e0-91df-6574d6289f32\" # first ID - llama 2-7b\n",
    "# UNIQUE_ID = \"20250703_063208_fb2466b9-2b8d-41d4-808c-bf702f90f6b1\" # first vicuna-7b only id\n",
    "# UNIQUE_ID = \"20250703_072510_971007bb-1b38-486c-9985-efadec6d0261\" # first vicuna-13b only id\n",
    "# UNIQUE_ID = \"20250703_190921_7d1d49ed-0de1-4f9e-b032-827c782d27fd\" # first mistral-7b only id\n",
    "# UNIQUE_ID = \"20250703_203613_b98fc3fb-905d-482e-ab74-17f74c55ae70\" # first llama 2-7b with null\n",
    "# UNIQUE_ID = \"20250705_155351_4bcd1439-3135-41b2-9018-f6d0514e8123\" # second Mistral (same results)\n",
    "# UNIQUE_ID = \"20250706_204304_b3d710a9-e68a-47cf-a433-eb5b1362205d\" # first deepseek\n",
    "# UNIQUE_ID = \"20250708_001549_61dc4b6d-d91a-430e-b9b4-c17120d88cd1\" # first llama 2-7b with different layers\n",
    "# UNIQUE_ID = \"20250708_192738_82b0cfe7-f603-4a5d-9fd3-39e636a4e51d\"\n",
    "DECOMP_CKPT = Path(f\"./checkpoints/decomposer_simple/decomposer_{UNIQUE_ID}\")\n",
    "\n",
    "cfg_unique_id = UNIQUE_ID\n",
    "cfg_unique_id = \"20250719_003746_3355037c-77e8-4595-85e9-7b1fd94e8bff\"\n",
    "CFG_OUT     = Path(f\"output/config_{cfg_unique_id}.yaml\")\n",
    "\n",
    "# ——— Load configs ———\n",
    "with open(CFG_OUT, 'r') as f:\n",
    "    cfg_out = yaml.safe_load(f)\n",
    "with open(CFG_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# override any layer settings from the output config\n",
    "config['model']['layers']         = cfg_out['model'].get('layers', 'last')\n",
    "config['model']['layer_combine']  = cfg_out['model'].get('layer_combine', 'mean')\n",
    "config['d_g'] = cfg_out['d_g']\n",
    "config['d_f'] = cfg_out['d_f']\n",
    "# config['hidden_dim'] = cfg_out['hidden_dim']\n",
    "ENC_LLM_NAME = cfg_out['model']['name']\n",
    "\n",
    "# ——— Logging & seeds ———\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s — %(name)s — %(levelname)s — %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "seed = config['experiment']['seed']\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "set_seed(seed)\n",
    "\n",
    "logger.info(\"Session setup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bff442",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf6ac7",
   "metadata": {},
   "source": [
    "### Split again and reorganize by cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f070c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell: [Data Loading & Preprocessing] ====\n",
    "\n",
    "def load_jsonl(path: str):\n",
    "    with open(path, 'r') as f:\n",
    "        return [\n",
    "            json.loads(line)\n",
    "            for line in f\n",
    "            if line.strip() and not line.strip().startswith('#')\n",
    "        ]\n",
    "\n",
    "# paths from config\n",
    "data_cfg = config['data']\n",
    "rawF_id            = load_jsonl(data_cfg[\"input_path_varyFraming_id\"])\n",
    "rawG_id            = load_jsonl(data_cfg[\"input_path_varyGoal_id\"])\n",
    "rawF_benign_id     = load_jsonl(data_cfg[\"input_path_varyFraming_benign_id\"])\n",
    "rawG_benign_id     = load_jsonl(data_cfg[\"input_path_varyGoal_benign_id\"])\n",
    "rawF_ood           = load_jsonl(data_cfg[\"input_path_varyFraming_ood\"])\n",
    "rawG_ood           = load_jsonl(data_cfg[\"input_path_varyGoal_ood\"])\n",
    "rawF_benign_ood    = load_jsonl(data_cfg[\"input_path_varyFraming_benign_ood\"])\n",
    "rawG_benign_ood    = load_jsonl(data_cfg[\"input_path_varyGoal_benign_ood\"])\n",
    "\n",
    "\n",
    "all_F_benign   = rawF_benign_id + rawF_benign_ood\n",
    "all_G_benign   = rawG_benign_id + rawG_benign_ood\n",
    "all_F_jailbrks = rawF_id        + rawF_ood\n",
    "all_G_jailbrks = rawG_id        + rawG_ood\n",
    "\n",
    "categories_F_benign = set([x['category'] for x in all_F_benign])\n",
    "categories_G_benign = set([x['category'] for x in all_G_benign])\n",
    "categories_F_jailbrks = set([x['category'] for x in all_F_jailbrks])\n",
    "categories_G_jailbrks = set([x['category'] for x in all_G_jailbrks])\n",
    "intersection = categories_F_benign.intersection(categories_G_benign)\n",
    "intersection = intersection.intersection(categories_F_jailbrks)\n",
    "intersection = intersection.intersection(categories_G_jailbrks)\n",
    "import random\n",
    "random.seed(seed)\n",
    "ood_cats = set(random.sample(list(intersection), len(intersection) // 3))\n",
    "id_cats = intersection.difference(ood_cats)\n",
    "\n",
    "\n",
    "rawF_id            = [entry for entry in rawF_id\n",
    "                      if entry['category'] in id_cats]\n",
    "rawG_id            = [entry for entry in rawG_id\n",
    "                      if entry['category'] in id_cats]\n",
    "rawF_benign_id     = [entry for entry in rawF_benign_id\n",
    "                            if entry['category'] in id_cats]\n",
    "rawG_benign_id     = [entry for entry in rawG_benign_id\n",
    "                      if entry['category'] in id_cats]\n",
    "rawF_ood           = [entry for entry in rawF_ood\n",
    "                      if entry['category'] in ood_cats]\n",
    "rawG_ood           = [entry for entry in rawG_ood\n",
    "                      if entry['category'] in ood_cats]\n",
    "rawF_benign_ood    = [entry for entry in rawF_benign_ood\n",
    "                            if entry['category'] in ood_cats]\n",
    "rawG_benign_ood    = [entry for entry in rawG_benign_ood\n",
    "                      if entry['category'] in ood_cats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a2dfad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and flag\n",
    "benign_id    = rawF_benign_id + rawG_benign_id\n",
    "jailbrks_id  = rawF_id          + rawG_id\n",
    "benign_ood   = rawF_benign_ood  + rawG_benign_ood\n",
    "jailbrks_ood = rawF_ood         + rawG_ood\n",
    "\n",
    "print(f\"{len(benign_id)=}, {len(jailbrks_id)=}\")\n",
    "print(f\"{len(benign_ood)=}, {len(jailbrks_ood)=}\")\n",
    "\n",
    "# balance ID splits\n",
    "min_id = min(len(benign_id), len(jailbrks_id))\n",
    "benign_id   = random.sample(benign_id, min_id)\n",
    "jailbrks_id = random.sample(jailbrks_id, min_id)\n",
    "\n",
    "# extract raw prompts\n",
    "get_prompts = lambda items: [e[\"prompt\"] for e in items]\n",
    "ben_ID   = get_prompts(benign_id)\n",
    "jb_ID    = get_prompts(jailbrks_id)\n",
    "ben_OOD  = get_prompts(benign_ood)\n",
    "jb_OOD   = get_prompts(jailbrks_ood)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fce2e6",
   "metadata": {},
   "source": [
    "### Keep the same splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "667a2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell: [Data Loading & Preprocessing] ====\n",
    "\n",
    "def load_jsonl(path: str):\n",
    "    with open(path, 'r') as f:\n",
    "        return [\n",
    "            json.loads(line)\n",
    "            for line in f\n",
    "            if line.strip() and not line.strip().startswith('#')\n",
    "        ]\n",
    "\n",
    "# paths from config\n",
    "data_cfg = config['data']\n",
    "rawF_id            = load_jsonl(data_cfg[\"input_path_varyFraming_id\"])\n",
    "rawG_id            = load_jsonl(data_cfg[\"input_path_varyGoal_id\"])\n",
    "rawF_benign_id     = load_jsonl(data_cfg[\"input_path_varyFraming_benign_id\"])\n",
    "rawG_benign_id     = load_jsonl(data_cfg[\"input_path_varyGoal_benign_id\"])\n",
    "rawF_ood           = load_jsonl(data_cfg[\"input_path_varyFraming_ood\"])\n",
    "rawG_ood           = load_jsonl(data_cfg[\"input_path_varyGoal_ood\"])\n",
    "rawF_benign_ood    = load_jsonl(data_cfg[\"input_path_varyFraming_benign_ood\"])\n",
    "rawG_benign_ood    = load_jsonl(data_cfg[\"input_path_varyGoal_benign_ood\"])\n",
    "\n",
    "# combine and flag\n",
    "benign_id    = rawF_benign_id + rawG_benign_id\n",
    "jailbrks_id  = rawF_id          + rawG_id\n",
    "benign_ood   = rawF_benign_ood  + rawG_benign_ood\n",
    "jailbrks_ood = rawF_ood         + rawG_ood\n",
    "\n",
    "print(f\"{len(benign_id)=}, {len(jailbrks_id)=}\")\n",
    "print(f\"{len(benign_ood)=}, {len(jailbrks_ood)=}\")\n",
    "\n",
    "# balance ID splits\n",
    "min_id = min(len(benign_id), len(jailbrks_id))\n",
    "benign_id   = random.sample(benign_id, min_id)\n",
    "jailbrks_id = random.sample(jailbrks_id, min_id)\n",
    "\n",
    "# extract raw prompts\n",
    "get_prompts = lambda items: [e[\"prompt\"] for e in items]\n",
    "ben_ID   = get_prompts(benign_id)\n",
    "jb_ID    = get_prompts(jailbrks_id)\n",
    "ben_OOD  = get_prompts(benign_ood)\n",
    "jb_OOD   = get_prompts(jailbrks_ood)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a768fb",
   "metadata": {},
   "source": [
    "#### Load original PAIR prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c107186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def load_all_pair_prompts(black_box_dir=\"./data/artifacts/attack-artifacts/PAIR/black_box/\"):\n",
    "    prompts = []\n",
    "    for file in Path(black_box_dir).glob(\"*.json\"):\n",
    "        with open(file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            for jb in data.get(\"jailbreaks\", []):\n",
    "                if jb.get(\"prompt\") is not None:\n",
    "                    prompts.append(jb[\"prompt\"])\n",
    "    return prompts\n",
    "\n",
    "all_pair_prompts = load_all_pair_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121948f8",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "606defea",
   "metadata": {},
   "outputs": [],
   "source": [
    "DECOMP_CKPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a8679c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==== Cell: [Model & Decomposer Initialization] ====\n",
    "\n",
    "# load LLM encoder\n",
    "model_llm, tokenizer = load_model(ENC_LLM_NAME, device=device)\n",
    "encoder = HFEncoder_notPooled(\n",
    "    model=model_llm,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    layers=\"last\",#config['model']['layers'],\n",
    "    layer_combine=config['model']['layer_combine'],\n",
    ")\n",
    "\n",
    "# # load decomposer weights\n",
    "# ckpt = torch.load(DECOMP_CKPT / \"weights.pt\", map_location=device)\n",
    "# enc_dim_ckpt = ckpt[\"Wg.0.weight\"].shape[1]\n",
    "\n",
    "# decomposer = NonlinearDecomposer(\n",
    "#     enc_dim    = enc_dim_ckpt,\n",
    "#     d_g        = config['d_g'],\n",
    "#     d_f        = config['d_f'],\n",
    "#     hidden_dim = config.get('hidden_dim', 1024),\n",
    "#     dropout    = config.get('dropout', 0.1),\n",
    "# ).to(device)\n",
    "# # decomposer = NonlinearDecomposer_tiny(\n",
    "# #     enc_dim    = enc_dim_ckpt,\n",
    "# # ).to(device)\n",
    "\n",
    "# decomposer.load_state_dict(ckpt)\n",
    "# decomposer.half().eval()\n",
    "# decomposer.eval()\n",
    "# encoder.eval()\n",
    "\n",
    "# logger.info(f\"Loaded encoder + decomposer (enc_dim={enc_dim_ckpt}).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b918293",
   "metadata": {},
   "source": [
    "#### Find critical layer - nsp dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ddd9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID_dec = \"20250719\" # UNIQUE_ID\n",
    "import re, glob, os\n",
    "\n",
    "pattern = re.compile(r\"decomposer_layer(\\d+)_\")\n",
    "ckpt_by_layer = {}\n",
    "for p in glob.glob(f\"./checkpoints/decomposer_simple/decomposer_layer*{RUN_ID_dec}*\"):\n",
    "    m = pattern.search(os.path.basename(p))\n",
    "    if m: ckpt_by_layer[int(m.group(1))] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0477711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CAL = 100              # or leave tunable in YAML\n",
    "cal_benign   = random.sample(ben_ID,    N_CAL//2)\n",
    "cal_jailbreak= random.sample(jb_ID,  N_CAL//2)\n",
    "\n",
    "from utils.critial_layer import find_critical_layers, find_critical_layers_dist\n",
    "\n",
    "layers = config[\"model\"][\"layers\"]\n",
    "if layers == 'all':\n",
    "    num_layers = model_llm.config.num_hidden_layers\n",
    "    layers = list(range(num_layers))\n",
    "\n",
    "cl_outs = dict()\n",
    "best_g, best_f = dict(), dict()\n",
    "for layer in layers:\n",
    "    # ==== Cell: [Model & Decomposer Initialization] ====\n",
    "\n",
    "    # load decomposer weights\n",
    "    ckpt = torch.load(f\"{ckpt_by_layer[layer]}/weights.pt\", map_location=device)\n",
    "    enc_dim_ckpt = ckpt[\"Wg.0.weight\"].shape[1]\n",
    "\n",
    "    decomposer = NonlinearDecomposer(\n",
    "        enc_dim    = enc_dim_ckpt,\n",
    "        d_g        = config['d_g'],\n",
    "        d_f        = config['d_f'],\n",
    "        hidden_dim = config.get('hidden_dim', 1024),\n",
    "        dropout    = config.get('dropout', 0.1),\n",
    "    ).to(device)\n",
    "\n",
    "    decomposer.load_state_dict(ckpt)\n",
    "    decomposer.half().eval()\n",
    "    decomposer.eval()\n",
    "    \n",
    "    # logger.info(f\"Loaded encoder + decomposer (enc_dim={enc_dim_ckpt}).\")\n",
    "    print(f\"\\n\\n\\nlayer {layer}:\\n\")\n",
    "    with torch.no_grad():\n",
    "        cl_outs[layer] = find_critical_layers_dist(HFEncoder_notPooled, model_llm, tokenizer, device, \n",
    "                                                decomposer, cal_benign, cal_jailbreak, [layer],\n",
    "                                                criterion='cohen_d')\n",
    "        best_g[layer] = {\"encoder_l\": cl_outs[layer][\"best_layer_goal\"], \"score\": cl_outs[layer][\"score_goal\"]}\n",
    "        best_f[layer] = {\"encoder_l\": cl_outs[layer][\"best_layer_framing\"], \"score\": cl_outs[layer][\"score_framing\"]}\n",
    "        print(f\"Best G: {best_g[layer]}, Best F: {best_f[layer]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488b84bc",
   "metadata": {},
   "source": [
    "#### Find critical layer - cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aa364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the first checkpoint path matching the pattern with \"layer\" in the name\n",
    "# import glob\n",
    "# ckpt_candidates = glob.glob(\"./checkpoints/decomposer_simple/decomposer_*layer*\")\n",
    "\n",
    "import re, glob, os\n",
    "\n",
    "pattern = re.compile(r\"decomposer_layer(\\d+)_\")\n",
    "ckpt_by_layer = {}\n",
    "for p in glob.glob(\"./checkpoints/decomposer_simple/decomposer_layer*\"):\n",
    "    m = pattern.search(os.path.basename(p))\n",
    "    if m: ckpt_by_layer[int(m.group(1))] = p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ad3441",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CAL = 100              # or leave tunable in YAML\n",
    "cal_benign   = random.sample(ben_ID,    N_CAL//2)\n",
    "cal_jailbreak= random.sample(jb_ID,  N_CAL//2)\n",
    "\n",
    "from utils.critial_layer import find_critical_layers\n",
    "\n",
    "layers = config[\"model\"][\"layers\"]\n",
    "if layers == 'all':\n",
    "    num_layers = model_llm.config.num_hidden_layers\n",
    "    layers = list(range(num_layers))\n",
    "\n",
    "cl_outs = dict()\n",
    "best_g, best_f = dict(), dict()\n",
    "for layer in layers:\n",
    "    # ==== Cell: [Model & Decomposer Initialization] ====\n",
    "\n",
    "    # # load LLM encoder\n",
    "    # model_llm, tokenizer = load_model(ENC_LLM_NAME, device=device)\n",
    "    # encoder = HFEncoder_notPooled(\n",
    "    #     model=model_llm,\n",
    "    #     tokenizer=tokenizer,\n",
    "    #     device=device,\n",
    "    #     layers=[layer],\n",
    "    #     layer_combine=config['model']['layer_combine'],\n",
    "    # )\n",
    "\n",
    "    # load decomposer weights\n",
    "    ckpt = torch.load(f\"{ckpt_by_layer[layer]}/weights.pt\", map_location=device)\n",
    "    enc_dim_ckpt = ckpt[\"Wg.0.weight\"].shape[1]\n",
    "\n",
    "    decomposer = NonlinearDecomposer(\n",
    "        enc_dim    = enc_dim_ckpt,\n",
    "        d_g        = config['d_g'],\n",
    "        d_f        = config['d_f'],\n",
    "        hidden_dim = config.get('hidden_dim', 1024),\n",
    "        dropout    = config.get('dropout', 0.1),\n",
    "    ).to(device)\n",
    "    # decomposer = NonlinearDecomposer_tiny(\n",
    "    #     enc_dim    = enc_dim_ckpt,\n",
    "    # ).to(device)\n",
    "\n",
    "    decomposer.load_state_dict(ckpt)\n",
    "    decomposer.half().eval()\n",
    "    decomposer.eval()\n",
    "    # encoder.eval()\n",
    "\n",
    "    # logger.info(f\"Loaded encoder + decomposer (enc_dim={enc_dim_ckpt}).\")\n",
    "    print(f\"\\n\\n\\nlayer {layer}:\\n\")\n",
    "    with torch.no_grad():\n",
    "        cl_outs[layer] = find_critical_layers(HFEncoder_notPooled, model_llm, tokenizer, device, \n",
    "                                                decomposer, cal_benign, cal_jailbreak, layers)\n",
    "        best_g[layer] = {\"encoder_l\": cl_outs[layer][\"l_g\"], \"Δ\": cl_outs[layer][\"Δ_g\"]}\n",
    "        best_f[layer] = {\"encoder_l\": cl_outs[layer][\"l_f\"], \"Δ\": cl_outs[layer][\"Δ_f\"]}\n",
    "        print(f\"Best G: {best_g[layer]}, Best F: {best_f[layer]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56aaff8",
   "metadata": {},
   "source": [
    "### Inspect each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9553d76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in best_f:\n",
    "    print(f\"\\nDec Layer {l}:\")\n",
    "    print(f\"G   |   enc layer: {best_g[l]['encoder_l']}, score: {best_g[l]['score']}\")\n",
    "    print(f\"F   |   enc layer: {best_f[l]['encoder_l']}, score: {best_f[l]['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0be51",
   "metadata": {},
   "source": [
    "### Load selected encoder and decomposer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba252798",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_g_tups = sorted(best_g.items(), key=lambda x: x[1]['score'], reverse=True)\n",
    "best_f_tups = sorted(best_f.items(), key=lambda x: x[1]['score'], reverse=True)\n",
    "num_layers = model_llm.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0117b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_layer = [l for l, _ in best_f_tups if l > num_layers//2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b2b898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52a8d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_layer =  7 # 25 # for f by cosine\n",
    "dec_layer = 7 # 25 # for f by cosine\n",
    "# enc_layer =  18 # for g by cosine\n",
    "# dec_layer = 18 # for g by cosine\n",
    "\n",
    "enc_layer =  17# 17 # 9 # for f by cohen_d\n",
    "dec_layer = 17# 17 # 9 # for f by cohen_d\n",
    "# enc_layer =  9 # 9 # for f by cohen_d\n",
    "# dec_layer = 9 # 9 # for f by cohen_d\n",
    "# # enc_layer =  5 # 17 # for g by cohen_d\n",
    "# # dec_layer = 5 # 17 # for g by cohen_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f888292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a03115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the first checkpoint path matching the pattern with \"layer\" in the name\n",
    "# import glob\n",
    "# ckpt_candidates = glob.glob(\"./checkpoints/decomposer_simple/decomposer_*layer*\")\n",
    "# # Find the first checkpoint path matching the pattern with \"layer\" in the name\n",
    "# import glob\n",
    "# ckpt_candidates = glob.glob(\"./checkpoints/decomposer_simple/decomposer_*layer*\")\n",
    "\n",
    "RUN_ID_dec = \"20250719\" # UNIQUE_ID\n",
    "import re, glob, os\n",
    "\n",
    "pattern = re.compile(r\"decomposer_layer(\\d+)_\")\n",
    "ckpt_by_layer = {}\n",
    "for p in glob.glob(f\"./checkpoints/decomposer_simple/decomposer_layer*{RUN_ID_dec}*\"):\n",
    "    m = pattern.search(os.path.basename(p))\n",
    "    if m: ckpt_by_layer[int(m.group(1))] = p\n",
    "\n",
    "# load LLM encoder\n",
    "model_llm, tokenizer = load_model(ENC_LLM_NAME, device=device)\n",
    "encoder = HFEncoder_notPooled(\n",
    "    model=model_llm,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    layers=[enc_layer],\n",
    "    layer_combine=config['model']['layer_combine'],\n",
    ")\n",
    "\n",
    "# load decomposer weights\n",
    "ckpt = torch.load(f\"{ckpt_by_layer[dec_layer]}/weights.pt\", map_location=device)\n",
    "enc_dim_ckpt = ckpt[\"Wg.0.weight\"].shape[1]\n",
    "\n",
    "decomposer = NonlinearDecomposer(\n",
    "    enc_dim    = enc_dim_ckpt,\n",
    "    d_g        = config['d_g'],\n",
    "    d_f        = config['d_f'],\n",
    "    hidden_dim = config.get('hidden_dim', 1024),\n",
    "    dropout    = config.get('dropout', 0.1),\n",
    ").to(device)\n",
    "# decomposer = NonlinearDecomposer_tiny(\n",
    "#     enc_dim    = enc_dim_ckpt,\n",
    "# ).to(device)\n",
    "\n",
    "decomposer.load_state_dict(ckpt)\n",
    "decomposer.half().eval()\n",
    "decomposer.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77267814",
   "metadata": {},
   "source": [
    "## Build spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "789187a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECT_VIA_FRAMING = True\n",
    "\n",
    "# if DETECT_VIA_FRAMING:\n",
    "#     enc_layer = [l for l, _ in best_f_tups if l > num_layers//2][0]\n",
    "# else:\n",
    "#     enc_layer = [l for l, _ in best_g_tups if l > num_layers//2][0]\n",
    "# dec_layer = enc_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40ade692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell: [Build Framing Vectors] ====\n",
    "\n",
    "@torch.no_grad()\n",
    "def framing_vecs(texts: list[str], batch_size: int = 32) -> torch.Tensor:\n",
    "    all_v = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        rep   = encoder(batch)               # (B, D_enc) or (B, T, D_enc)\n",
    "        _, v_f, _ = decomposer(rep)          # framing component\n",
    "        if v_f.dim() == 3:                   # token-wise\n",
    "            v_f = v_f.mean(dim=1)\n",
    "        all_v.append(v_f.cpu())\n",
    "    return torch.cat(all_v, dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def goal_vecs(texts: list[str], batch_size: int = 32) -> torch.Tensor:\n",
    "    all_v = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        rep   = encoder(batch)               # (B, D_enc) or (B, T, D_enc)\n",
    "        v_g, _, _ = decomposer(rep)          # framing component\n",
    "        if v_g.dim() == 3:                   # token-wise\n",
    "            v_g = v_g.mean(dim=1)\n",
    "        all_v.append(v_g.cpu())\n",
    "    return torch.cat(all_v, dim=0)\n",
    "\n",
    "\n",
    "if DETECT_VIA_FRAMING:\n",
    "    v_ben_ID  = framing_vecs(ben_ID)\n",
    "    v_jb_ID   = framing_vecs(jb_ID)\n",
    "    v_ben_OOD = framing_vecs(ben_OOD)\n",
    "    v_jb_OOD  = framing_vecs(jb_OOD)\n",
    "    logger.info(\"Built framing vectors for all splits.\")\n",
    "else:\n",
    "    v_ben_ID  = goal_vecs(ben_ID)\n",
    "    v_jb_ID   = goal_vecs(jb_ID)\n",
    "    v_ben_OOD = goal_vecs(ben_OOD)\n",
    "    v_jb_OOD  = goal_vecs(jb_OOD)\n",
    "    logger.info(\"Built goal vectors for all splits.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c480584",
   "metadata": {},
   "source": [
    "### Whiten and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9bf576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import LedoitWolf#, EmpiricalCovariance\n",
    "\n",
    "\n",
    "\n",
    "def fit_whiten_pca(X: np.ndarray, alpha: float = 0.9):\n",
    "    mu   = X.mean(axis=0, keepdims=True)\n",
    "    Xc   = X - mu\n",
    "    cov  = np.cov(Xc, rowvar=False) + 1e-5*np.eye(X.shape[1])\n",
    "    # lwcov = LedoitWolf()\n",
    "    # cov = lwcov.fit(X)\n",
    "    vals, vecs = np.linalg.eigh(cov)\n",
    "    W    = vecs @ np.diag(vals**-0.5) @ vecs.T\n",
    "    Z    = Xc @ W.T\n",
    "\n",
    "    pca  = PCA(whiten=False).fit(Z)\n",
    "    cum  = np.cumsum(pca.explained_variance_ratio_)\n",
    "    r    = int(np.searchsorted(cum, alpha)) + 1\n",
    "    P    = pca.components_[:r].T\n",
    "\n",
    "    return {\"mu\": mu, \"W\": W, \"P\": P}\n",
    "\n",
    "# fit detector on benign-ID\n",
    "# detector = fit_whiten_pca(v_ben_ID.numpy(), alpha=0.9)\n",
    "# alphas = [.5,.6,0.7,0.8,0.9,.95,.99]\n",
    "alphas = [0.7,0.9]\n",
    "detectors = {a: fit_whiten_pca(v_ben_ID.numpy(), alpha=a) for a in alphas}\n",
    "\n",
    "# pickle.dump(detector, open(\"checkpoints/nsp_detector.pkl\",\"wb\"))\n",
    "\n",
    "# precompute benign centroid\n",
    "def residual_vec(V: np.ndarray, det):\n",
    "    z    = (V - det[\"mu\"]) @ det[\"W\"].T\n",
    "    proj = det[\"P\"] @ (det[\"P\"].T @ z.T)\n",
    "    return z - proj.T\n",
    "\n",
    "R_ben_IDs = {a: residual_vec(v_ben_ID.numpy(), detectors[a])\n",
    "             for a in detectors}\n",
    "R_ben_IDs = {a: R_ben_IDs[a] / (np.linalg.norm(R_ben_IDs[a], axis=1, keepdims=True) + 1e-9)\n",
    "             for a in R_ben_IDs}\n",
    "centroids = {a: R_ben_IDs[a].mean(axis=0, keepdims=True)\n",
    "             for a in R_ben_IDs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a279e10f",
   "metadata": {},
   "source": [
    "### Mahalanobis score (as an alternative to whitening and PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf97747e",
   "metadata": {},
   "source": [
    "### With cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbb854d",
   "metadata": {},
   "source": [
    "#### Without Val Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7708fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell: [Detection – Cosine-based NSP] ====\n",
    "def cos_score(V: np.ndarray):\n",
    "    Rs = {a: residual_vec(V, detectors[a]) \n",
    "          for a in detectors}\n",
    "    Rs = {a: Rs[a] / (np.linalg.norm(Rs[a], axis=1, keepdims=True) + 1e-9)\n",
    "          for a in Rs}\n",
    "    return {a: 1.0 - (Rs[a] * centroids[a]).sum(axis=1) for a in Rs}\n",
    "\n",
    "# threshold\n",
    "taus = {a: np.percentile(cos_score(v_ben_ID.numpy())[a], 95)\n",
    "       for a in detectors}\n",
    "for a in taus:\n",
    "    print(f\"cosine-score τ (95% benign-ID with alpha={a}) = {taus[a]:.4f}\")\n",
    "\n",
    "# evaluation\n",
    "def eval_split(v_ben, v_jb, name):\n",
    "    y    = np.concatenate([np.zeros(len(v_ben)), np.ones(len(v_jb))])\n",
    "    s    = {a: np.concatenate([cos_score(v_ben.numpy())[a],\n",
    "                                cos_score(v_jb.numpy())[a]])\n",
    "            for a in alphas}\n",
    "    au   = {a: roc_auc_score(y, s[a])\n",
    "            for a in s}\n",
    "    tprs  = {a: (s[a][len(v_ben):] > taus[a]).mean() for a in taus}\n",
    "    fprs  = {a: (s[a][:len(v_ben)] > taus[a]).mean() for a in taus}\n",
    "    for a in taus:\n",
    "        print(f\"alpha={a:>2} , {name:>6} | AUROC {au[a]:.3f}  TPR@τ {tprs[a]:.3f}  FPR@τ {fprs[a]:.3f}\")\n",
    "\n",
    "eval_split(v_ben_ID,  v_jb_ID,  \"ID\")\n",
    "eval_split(v_ben_OOD, v_jb_OOD, \"OOD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25e9040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def acc_f1(v_ben, v_jb, name):\n",
    "    y_true = np.concatenate([np.zeros(len(v_ben)), np.ones(len(v_jb))])\n",
    "    scores = {a: np.concatenate([cos_score(v_ben.numpy())[a],\n",
    "                                cos_score(v_jb.numpy())[a]])\n",
    "              for a in alphas\n",
    "    }\n",
    "    y_pred = {a: (scores[a] > taus[a]).astype(int) for a in taus}\n",
    "    accs = {a: accuracy_score(y_true, y_pred[a]) for a in taus}\n",
    "    prec_recall_f1 = {a: precision_recall_fscore_support(\n",
    "        y_true, y_pred[a], average=\"binary\", pos_label=1, zero_division=0\n",
    "    ) for a in taus}\n",
    "    for a in taus:\n",
    "        print(f\"alpha={a:>2} , {name:>6} | Acc {accs[a]:.3f}  Prec {prec_recall_f1[a][0]:.3f}  Rec {prec_recall_f1[a][1]:.3f}  F1 {prec_recall_f1[a][2]:.3f}\")\n",
    "\n",
    "acc_f1(v_ben_ID,  v_jb_ID,  \"ID\")\n",
    "acc_f1(v_ben_OOD, v_jb_OOD, \"OOD\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c6c8c8",
   "metadata": {},
   "source": [
    "### With L2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a935594",
   "metadata": {},
   "source": [
    "#### With Val Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "121e6fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell: [Detection – χ²-based L2 NSP (with validation split)] ====\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# 1) Split benign ID into train / val\n",
    "ben_ID_train, ben_ID_val = train_test_split(v_ben_ID, test_size=0.25, random_state=42)\n",
    "jb_ID_train,  jb_ID_val  = train_test_split(v_jb_ID,  test_size=0.25, random_state=42)\n",
    "\n",
    "# 2) Fit detector on train split\n",
    "# detector_l2_val = fit_whiten_pca(ben_ID_train.numpy(), alpha=0.9)\n",
    "# alphas = [0.7,.8, .85, 0.9, .95]\n",
    "alphas = [0.7, 0.9]\n",
    "detectors_l2_val = {a: fit_whiten_pca(ben_ID_train.numpy(), alpha=a) for a in alphas}\n",
    "\n",
    "\n",
    "# 3) χ² threshold (squared-L2 residual)\n",
    "chis_df_val = {a: ben_ID_train.shape[1] - detectors_l2_val[a][\"P\"].shape[1] for a in alphas}\n",
    "taus_l2_val = {a: chi2.ppf(0.95, df=chis_df_val[a]) ** 0.5 for a in alphas}\n",
    "for a in alphas:    \n",
    "    print(f\"χ²-based τ (with val, alpha={a}) = {taus_l2_val[a]:.4f}\")\n",
    "\n",
    "# 4) Scoring & evaluation\n",
    "def nsp_score_l2(V: np.ndarray, det) -> np.ndarray:\n",
    "    res = residual_vec(V, det)\n",
    "    return np.linalg.norm(res, axis=1)\n",
    "\n",
    "def eval_l2_with_val(v_ben, v_jb, name):\n",
    "    y_true = np.concatenate([np.zeros(len(v_ben)), np.ones(len(v_jb))])\n",
    "    scores = {a: np.concatenate([\n",
    "        nsp_score_l2(v_ben.numpy(), detectors_l2_val[a]),\n",
    "        nsp_score_l2(v_jb.numpy(), detectors_l2_val[a]),\n",
    "    ]) for a in detectors_l2_val}\n",
    "    y_pred = {a: (scores[a] > taus_l2_val[a]).astype(int) for a in alphas}\n",
    "\n",
    "    au   = {a: roc_auc_score(y_true, scores[a]) for a in alphas}\n",
    "    prec_rec_f1_ = {a: precision_recall_fscore_support(\n",
    "        y_true, y_pred[a], average=\"binary\", pos_label=1, zero_division=0\n",
    "    ) for a in alphas}\n",
    "    accs = {a: accuracy_score(y_true, y_pred[a]) for a in alphas}\n",
    "    tprs = {a: y_pred[a][len(v_ben):].mean() for a in alphas}\n",
    "    fprs = {a: y_pred[a][:len(v_ben)].mean() for a in alphas}\n",
    "    fnrs = {a: (1 - y_pred[a][len(v_ben):]).mean() for a in alphas}\n",
    "    tnrs = {a: (1 - y_pred[a][:len(v_ben)]).mean() for a in alphas}\n",
    "    for a in alphas:\n",
    "        print(f\"alpha={a:>2} , {name:>6} | Acc {accs[a]:.3f} F1 {prec_rec_f1_[a][2]:.3f} AUROC {au[a]:.3f} Prec {prec_rec_f1_[a][0]:.3f}  Rec {prec_rec_f1_[a][1]:.3f} TPR {tprs[a]:.3f} FPR {fprs[a]:.3f} TNR {tnrs[a]:.3f} FNR {fnrs[a]:.3f}\")\n",
    "\n",
    "# Eval on held-out ID val and full OOD\n",
    "eval_l2_with_val(ben_ID_val,  jb_ID_val,  \"ID-val\")\n",
    "eval_l2_with_val(v_ben_OOD,  v_jb_OOD,  \"OOD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c880eb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell: [Detection – χ²-based L2 NSP (with validation split)] ====\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# 1) Split benign ID into train / val\n",
    "ben_ID_train, ben_ID_val = train_test_split(v_ben_ID, test_size=0.2, random_state=42)\n",
    "jb_ID_train,  jb_ID_val  = train_test_split(v_jb_ID,  test_size=0.2, random_state=42)\n",
    "\n",
    "# 2) Fit detector on train split\n",
    "# detector_l2_val = fit_whiten_pca(ben_ID_train.numpy(), alpha=0.9)\n",
    "# alphas = [0.7,.8, .85, 0.9, .95]\n",
    "alphas = [0.7, 0.9]\n",
    "detectors_l2_val = {a: fit_whiten_pca(ben_ID_train.numpy(), alpha=a) for a in alphas}\n",
    "\n",
    "\n",
    "# 3) χ² threshold (squared-L2 residual)\n",
    "chis_df_val = {a: ben_ID_train.shape[1] - detectors_l2_val[a][\"P\"].shape[1] for a in alphas}\n",
    "taus_l2_val = {a: chi2.ppf(0.95, df=chis_df_val[a]) ** 0.5 for a in alphas}\n",
    "for a in alphas:    \n",
    "    print(f\"χ²-based τ (with val, alpha={a}) = {taus_l2_val[a]:.4f}\")\n",
    "\n",
    "# 4) Scoring & evaluation\n",
    "def nsp_score_l2(V: np.ndarray, det) -> np.ndarray:\n",
    "    res = residual_vec(V, det)\n",
    "    return np.linalg.norm(res, axis=1)\n",
    "\n",
    "def eval_l2_with_val(v_ben, v_jb, name):\n",
    "    y_true = np.concatenate([np.zeros(len(v_ben)), np.ones(len(v_jb))])\n",
    "    scores = {a: np.concatenate([\n",
    "        nsp_score_l2(v_ben.numpy(), detectors_l2_val[a]),\n",
    "        nsp_score_l2(v_jb.numpy(), detectors_l2_val[a]),\n",
    "    ]) for a in detectors_l2_val}\n",
    "    y_pred = {a: (scores[a] > taus_l2_val[a]).astype(int) for a in alphas}\n",
    "\n",
    "    au   = {a: roc_auc_score(y_true, scores[a]) for a in alphas}\n",
    "    prec_rec_f1_ = {a: precision_recall_fscore_support(\n",
    "        y_true, y_pred[a], average=\"binary\", pos_label=1, zero_division=0\n",
    "    ) for a in alphas}\n",
    "    accs = {a: accuracy_score(y_true, y_pred[a]) for a in alphas}\n",
    "    tprs = {a: y_pred[a][len(v_ben):].mean() for a in alphas}\n",
    "    fprs = {a: y_pred[a][:len(v_ben)].mean() for a in alphas}\n",
    "    fnrs = {a: (1 - y_pred[a][len(v_ben):]).mean() for a in alphas}\n",
    "    tnrs = {a: (1 - y_pred[a][:len(v_ben)]).mean() for a in alphas}\n",
    "    for a in alphas:\n",
    "        print(f\"alpha={a:>2} , {name:>6} | Acc {accs[a]:.3f} F1 {prec_rec_f1_[a][2]:.3f} AUROC {au[a]:.3f} Prec {prec_rec_f1_[a][0]:.3f}  Rec {prec_rec_f1_[a][1]:.3f} TPR {tprs[a]:.3f} FPR {fprs[a]:.3f} TNR {tnrs[a]:.3f} FNR {fnrs[a]:.3f}\")\n",
    "\n",
    "# Eval on held-out ID val and full OOD\n",
    "eval_l2_with_val(ben_ID_val,  jb_ID_val,  \"ID-val\")\n",
    "eval_l2_with_val(v_ben_OOD,  v_jb_OOD,  \"OOD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d608243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell: [Detection – χ²-based L2 NSP (with validation split)] ====\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# 1) Split benign ID into train / val\n",
    "ben_ID_train, ben_ID_val = train_test_split(v_ben_ID, test_size=0.2, random_state=42)\n",
    "jb_ID_train,  jb_ID_val  = train_test_split(v_jb_ID,  test_size=0.2, random_state=42)\n",
    "\n",
    "# 2) Fit detector on train split\n",
    "# detector_l2_val = fit_whiten_pca(ben_ID_train.numpy(), alpha=0.9)\n",
    "# alphas = [0.7,.8, .85, 0.9, .95]\n",
    "alphas = [.6, 0.7, 0.9]\n",
    "detectors_l2_val = {a: fit_whiten_pca(ben_ID_train.numpy(), alpha=a) for a in alphas}\n",
    "\n",
    "\n",
    "# 3) χ² threshold (squared-L2 residual)\n",
    "chis_df_val = {a: ben_ID_train.shape[1] - detectors_l2_val[a][\"P\"].shape[1] for a in alphas}\n",
    "taus_l2_val = {a: chi2.ppf(0.95, df=chis_df_val[a]) ** 0.5 for a in alphas}\n",
    "for a in alphas:    \n",
    "    print(f\"χ²-based τ (with val, alpha={a}) = {taus_l2_val[a]:.4f}\")\n",
    "\n",
    "# 4) Scoring & evaluation\n",
    "def nsp_score_l2(V: np.ndarray, det) -> np.ndarray:\n",
    "    res = residual_vec(V, det)\n",
    "    return np.linalg.norm(res, axis=1)\n",
    "\n",
    "def eval_l2_with_val(v_ben, v_jb, name):\n",
    "    y_true = np.concatenate([np.zeros(len(v_ben)), np.ones(len(v_jb))])\n",
    "    scores = {a: np.concatenate([\n",
    "        nsp_score_l2(v_ben.numpy(), detectors_l2_val[a]),\n",
    "        nsp_score_l2(v_jb.numpy(), detectors_l2_val[a]),\n",
    "    ]) for a in detectors_l2_val}\n",
    "    y_pred = {a: (scores[a] > taus_l2_val[a]).astype(int) for a in alphas}\n",
    "\n",
    "    au   = {a: roc_auc_score(y_true, scores[a]) for a in alphas}\n",
    "    prec_rec_f1_ = {a: precision_recall_fscore_support(\n",
    "        y_true, y_pred[a], average=\"binary\", pos_label=1, zero_division=0\n",
    "    ) for a in alphas}\n",
    "    accs = {a: accuracy_score(y_true, y_pred[a]) for a in alphas}\n",
    "    tprs = {a: y_pred[a][len(v_ben):].mean() for a in alphas}\n",
    "    fprs = {a: y_pred[a][:len(v_ben)].mean() for a in alphas}\n",
    "    fnrs = {a: (1 - y_pred[a][len(v_ben):]).mean() for a in alphas}\n",
    "    tnrs = {a: (1 - y_pred[a][:len(v_ben)]).mean() for a in alphas}\n",
    "    for a in alphas:\n",
    "        print(f\"alpha={a:>2} , {name:>6} | Acc {accs[a]:.3f} F1 {prec_rec_f1_[a][2]:.3f} AUROC {au[a]:.3f} Prec {prec_rec_f1_[a][0]:.3f}  Rec {prec_rec_f1_[a][1]:.3f} TPR {tprs[a]:.3f} FPR {fprs[a]:.3f} TNR {tnrs[a]:.3f} FNR {fnrs[a]:.3f}\")\n",
    "\n",
    "# Eval on held-out ID val and full OOD\n",
    "eval_l2_with_val(ben_ID_val,  jb_ID_val,  \"ID-val\")\n",
    "eval_l2_with_val(v_ben_OOD,  v_jb_OOD,  \"OOD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9378a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell: [Detection – χ²-based L2 NSP (with validation split)] ====\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# 1) Split benign ID into train / val\n",
    "ben_ID_train, ben_ID_val = train_test_split(v_ben_ID, test_size=0.2, random_state=42)\n",
    "jb_ID_train,  jb_ID_val  = train_test_split(v_jb_ID,  test_size=0.2, random_state=42)\n",
    "\n",
    "# 2) Fit detector on train split\n",
    "# detector_l2_val = fit_whiten_pca(ben_ID_train.numpy(), alpha=0.9)\n",
    "# alphas = [0.7,.8, .85, 0.9, .95]\n",
    "alphas = [0.7, 0.9]\n",
    "detectors_l2_val = {a: fit_whiten_pca(ben_ID_train.numpy(), alpha=a) for a in alphas}\n",
    "\n",
    "\n",
    "# 3) χ² threshold (squared-L2 residual)\n",
    "chis_df_val = {a: ben_ID_train.shape[1] - detectors_l2_val[a][\"P\"].shape[1] for a in alphas}\n",
    "taus_l2_val = {a: chi2.ppf(0.95, df=chis_df_val[a]) ** 0.5 for a in alphas}\n",
    "for a in alphas:    \n",
    "    print(f\"χ²-based τ (with val, alpha={a}) = {taus_l2_val[a]:.4f}\")\n",
    "\n",
    "# 4) Scoring & evaluation\n",
    "def nsp_score_l2(V: np.ndarray, det) -> np.ndarray:\n",
    "    res = residual_vec(V, det)\n",
    "    return np.linalg.norm(res, axis=1)\n",
    "\n",
    "def eval_l2_with_val(v_ben, v_jb, name):\n",
    "    y_true = np.concatenate([np.zeros(len(v_ben)), np.ones(len(v_jb))])\n",
    "    scores = {a: np.concatenate([\n",
    "        nsp_score_l2(v_ben.numpy(), detectors_l2_val[a]),\n",
    "        nsp_score_l2(v_jb.numpy(), detectors_l2_val[a]),\n",
    "    ]) for a in detectors_l2_val}\n",
    "    y_pred = {a: (scores[a] > taus_l2_val[a]).astype(int) for a in alphas}\n",
    "\n",
    "    au   = {a: roc_auc_score(y_true, scores[a]) for a in alphas}\n",
    "    prec_rec_f1_ = {a: precision_recall_fscore_support(\n",
    "        y_true, y_pred[a], average=\"binary\", pos_label=1, zero_division=0\n",
    "    ) for a in alphas}\n",
    "    accs = {a: accuracy_score(y_true, y_pred[a]) for a in alphas}\n",
    "    tprs = {a: y_pred[a][len(v_ben):].mean() for a in alphas}\n",
    "    fprs = {a: y_pred[a][:len(v_ben)].mean() for a in alphas}\n",
    "    fnrs = {a: (1 - y_pred[a][len(v_ben):]).mean() for a in alphas}\n",
    "    tnrs = {a: (1 - y_pred[a][:len(v_ben)]).mean() for a in alphas}\n",
    "    for a in alphas:\n",
    "        print(f\"alpha={a:>2} , {name:>6} | Acc {accs[a]:.3f} F1 {prec_rec_f1_[a][2]:.3f} AUROC {au[a]:.3f} Prec {prec_rec_f1_[a][0]:.3f}  Rec {prec_rec_f1_[a][1]:.3f} TPR {tprs[a]:.3f} FPR {fprs[a]:.3f} TNR {tnrs[a]:.3f} FNR {fnrs[a]:.3f}\")\n",
    "\n",
    "# Eval on held-out ID val and full OOD\n",
    "eval_l2_with_val(ben_ID_val,  jb_ID_val,  \"ID-val\")\n",
    "eval_l2_with_val(v_ben_OOD,  v_jb_OOD,  \"OOD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde9f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell: [Detection – χ²-based L2 NSP (with validation split)] ====\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# 1) Split benign ID into train / val\n",
    "ben_ID_train, ben_ID_val = train_test_split(v_ben_ID, test_size=0.2, random_state=42)\n",
    "jb_ID_train,  jb_ID_val  = train_test_split(v_jb_ID,  test_size=0.2, random_state=42)\n",
    "\n",
    "# 2) Fit detector on train split\n",
    "# detector_l2_val = fit_whiten_pca(ben_ID_train.numpy(), alpha=0.9)\n",
    "alphas = [0.7,0.9]\n",
    "detectors_l2_val = {a: fit_whiten_pca(ben_ID_train.numpy(), alpha=a) for a in alphas}\n",
    "\n",
    "\n",
    "# 3) χ² threshold (squared-L2 residual)\n",
    "chis_df_val = {a: ben_ID_train.shape[1] - detectors_l2_val[a][\"P\"].shape[1] for a in alphas}\n",
    "taus_l2_val = {a: chi2.ppf(0.95, df=chis_df_val[a]) ** 0.5 for a in alphas}\n",
    "for a in alphas:    \n",
    "    print(f\"χ²-based τ (with val, alpha={a}) = {taus_l2_val[a]:.4f}\")\n",
    "\n",
    "# 4) Scoring & evaluation\n",
    "def nsp_score_l2(V: np.ndarray, det) -> np.ndarray:\n",
    "    res = residual_vec(V, det)\n",
    "    return np.linalg.norm(res, axis=1)\n",
    "\n",
    "def eval_l2_with_val(v_ben, v_jb, name):\n",
    "    y_true = np.concatenate([np.zeros(len(v_ben)), np.ones(len(v_jb))])\n",
    "    scores = {a: np.concatenate([\n",
    "        nsp_score_l2(v_ben.numpy(), detectors_l2_val[a]),\n",
    "        nsp_score_l2(v_jb.numpy(), detectors_l2_val[a]),\n",
    "    ]) for a in detectors_l2_val}\n",
    "    y_pred = {a: (scores[a] > taus_l2_val[a]).astype(int) for a in alphas}\n",
    "\n",
    "    au   = {a: roc_auc_score(y_true, scores[a]) for a in alphas}\n",
    "    prec_rec_f1_ = {a: precision_recall_fscore_support(\n",
    "        y_true, y_pred[a], average=\"binary\", pos_label=1, zero_division=0\n",
    "    ) for a in alphas}\n",
    "    accs = {a: accuracy_score(y_true, y_pred[a]) for a in alphas}\n",
    "    tprs = {a: y_pred[a][len(v_ben):].mean() for a in alphas}\n",
    "    fprs = {a: y_pred[a][:len(v_ben)].mean() for a in alphas}\n",
    "    fnrs = {a: (1 - y_pred[a][len(v_ben):]).mean() for a in alphas}\n",
    "    tnrs = {a: (1 - y_pred[a][:len(v_ben)]).mean() for a in alphas}\n",
    "    for a in alphas:\n",
    "        print(f\"alpha={a:>2} , {name:>6} | Acc {accs[a]:.3f} F1 {prec_rec_f1_[a][2]:.3f} AUROC {au[a]:.3f} Prec {prec_rec_f1_[a][0]:.3f}  Rec {prec_rec_f1_[a][1]:.3f} TPR {tprs[a]:.3f} FPR {fprs[a]:.3f} TNR {tnrs[a]:.3f} FNR {fnrs[a]:.3f}\")\n",
    "\n",
    "# Eval on held-out ID val and full OOD\n",
    "eval_l2_with_val(ben_ID_val,  jb_ID_val,  \"ID-val\")\n",
    "eval_l2_with_val(v_ben_OOD,  v_jb_OOD,  \"OOD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6551a47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell: [Detection – χ²-based L2 NSP (with validation split)] ====\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# 1) Split benign ID into train / val\n",
    "ben_ID_train, ben_ID_val = train_test_split(v_ben_ID, test_size=0.2, random_state=42)\n",
    "jb_ID_train,  jb_ID_val  = train_test_split(v_jb_ID,  test_size=0.2, random_state=42)\n",
    "\n",
    "# 2) Fit detector on train split\n",
    "# detector_l2_val = fit_whiten_pca(ben_ID_train.numpy(), alpha=0.9)\n",
    "alphas = [0.7,0.9]\n",
    "detectors_l2_val = {a: fit_whiten_pca(ben_ID_train.numpy(), alpha=a) for a in alphas}\n",
    "\n",
    "\n",
    "# 3) χ² threshold (squared-L2 residual)\n",
    "chis_df_val = {a: ben_ID_train.shape[1] - detectors_l2_val[a][\"P\"].shape[1] for a in alphas}\n",
    "taus_l2_val = {a: chi2.ppf(0.95, df=chis_df_val[a]) ** 0.5 for a in alphas}\n",
    "for a in alphas:    \n",
    "    print(f\"χ²-based τ (with val, alpha={a}) = {taus_l2_val[a]:.4f}\")\n",
    "\n",
    "# 4) Scoring & evaluation\n",
    "def nsp_score_l2(V: np.ndarray, det) -> np.ndarray:\n",
    "    res = residual_vec(V, det)\n",
    "    return np.linalg.norm(res, axis=1)\n",
    "\n",
    "def eval_l2_with_val(v_ben, v_jb, name):\n",
    "    y_true = np.concatenate([np.zeros(len(v_ben)), np.ones(len(v_jb))])\n",
    "    scores = {a: np.concatenate([\n",
    "        nsp_score_l2(v_ben.numpy(), detectors_l2_val[a]),\n",
    "        nsp_score_l2(v_jb.numpy(), detectors_l2_val[a]),\n",
    "    ]) for a in detectors_l2_val}\n",
    "    y_pred = {a: (scores[a] > taus_l2_val[a]).astype(int) for a in alphas}\n",
    "\n",
    "    au   = {a: roc_auc_score(y_true, scores[a]) for a in alphas}\n",
    "    prec_rec_f1_ = {a: precision_recall_fscore_support(\n",
    "        y_true, y_pred[a], average=\"binary\", pos_label=1, zero_division=0\n",
    "    ) for a in alphas}\n",
    "    accs = {a: accuracy_score(y_true, y_pred[a]) for a in alphas}\n",
    "    tprs = {a: y_pred[a][len(v_ben):].mean() for a in alphas}\n",
    "    fprs = {a: y_pred[a][:len(v_ben)].mean() for a in alphas}\n",
    "    fnrs = {a: (1 - y_pred[a][len(v_ben):]).mean() for a in alphas}\n",
    "    tnrs = {a: (1 - y_pred[a][:len(v_ben)]).mean() for a in alphas}\n",
    "    for a in alphas:\n",
    "        print(f\"alpha={a:>2} , {name:>6} | Acc {accs[a]:.3f} F1 {prec_rec_f1_[a][2]:.3f} AUROC {au[a]:.3f} Prec {prec_rec_f1_[a][0]:.3f}  Rec {prec_rec_f1_[a][1]:.3f} TPR {tprs[a]:.3f} FPR {fprs[a]:.3f} TNR {tnrs[a]:.3f} FNR {fnrs[a]:.3f}\")\n",
    "\n",
    "# Eval on held-out ID val and full OOD\n",
    "eval_l2_with_val(ben_ID_val,  jb_ID_val,  \"ID-val\")\n",
    "eval_l2_with_val(v_ben_OOD,  v_jb_OOD,  \"OOD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6727edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell: [Detection – χ²-based L2 NSP (with validation split)] ====\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# 1) Split benign ID into train / val\n",
    "ben_ID_train, ben_ID_val = train_test_split(v_ben_ID, test_size=0.2, random_state=42)\n",
    "jb_ID_train,  jb_ID_val  = train_test_split(v_jb_ID,  test_size=0.2, random_state=42)\n",
    "\n",
    "# 2) Fit detector on train split\n",
    "# detector_l2_val = fit_whiten_pca(ben_ID_train.numpy(), alpha=0.9)\n",
    "alphas = [0.7,0.9]\n",
    "detectors_l2_val = {a: fit_whiten_pca(ben_ID_train.numpy(), alpha=a) for a in alphas}\n",
    "\n",
    "\n",
    "# 3) χ² threshold (squared-L2 residual)\n",
    "chis_df_val = {a: ben_ID_train.shape[1] - detectors_l2_val[a][\"P\"].shape[1] for a in alphas}\n",
    "taus_l2_val = {a: chi2.ppf(0.95, df=chis_df_val[a]) ** 0.5 for a in alphas}\n",
    "for a in alphas:    \n",
    "    print(f\"χ²-based τ (with val, alpha={a}) = {taus_l2_val[a]:.4f}\")\n",
    "\n",
    "# 4) Scoring & evaluation\n",
    "def nsp_score_l2(V: np.ndarray, det) -> np.ndarray:\n",
    "    res = residual_vec(V, det)\n",
    "    return np.linalg.norm(res, axis=1)\n",
    "\n",
    "def eval_l2_with_val(v_ben, v_jb, name):\n",
    "    y_true = np.concatenate([np.zeros(len(v_ben)), np.ones(len(v_jb))])\n",
    "    scores = {a: np.concatenate([\n",
    "        nsp_score_l2(v_ben.numpy(), detectors_l2_val[a]),\n",
    "        nsp_score_l2(v_jb.numpy(), detectors_l2_val[a]),\n",
    "    ]) for a in detectors_l2_val}\n",
    "    y_pred = {a: (scores[a] > taus_l2_val[a]).astype(int) for a in alphas}\n",
    "\n",
    "    au   = {a: roc_auc_score(y_true, scores[a]) for a in alphas}\n",
    "    prec_rec_f1_ = {a: precision_recall_fscore_support(\n",
    "        y_true, y_pred[a], average=\"binary\", pos_label=1, zero_division=0\n",
    "    ) for a in alphas}\n",
    "    accs = {a: accuracy_score(y_true, y_pred[a]) for a in alphas}\n",
    "    tprs = {a: y_pred[a][len(v_ben):].mean() for a in alphas}\n",
    "    fprs = {a: y_pred[a][:len(v_ben)].mean() for a in alphas}\n",
    "    fnrs = {a: (1 - y_pred[a][len(v_ben):]).mean() for a in alphas}\n",
    "    tnrs = {a: (1 - y_pred[a][:len(v_ben)]).mean() for a in alphas}\n",
    "    for a in alphas:\n",
    "        print(f\"alpha={a:>2} , {name:>6} | Acc {accs[a]:.3f} F1 {prec_rec_f1_[a][2]:.3f} AUROC {au[a]:.3f} Prec {prec_rec_f1_[a][0]:.3f}  Rec {prec_rec_f1_[a][1]:.3f} TPR {tprs[a]:.3f} FPR {fprs[a]:.3f} TNR {tnrs[a]:.3f} FNR {fnrs[a]:.3f}\")\n",
    "\n",
    "# Eval on held-out ID val and full OOD\n",
    "eval_l2_with_val(ben_ID_val,  jb_ID_val,  \"ID-val\")\n",
    "eval_l2_with_val(v_ben_OOD,  v_jb_OOD,  \"OOD\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f1f32d",
   "metadata": {},
   "source": [
    "#### Without Val Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b376508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell: [Detection – χ²-based L2 NSP (without validation split)] ====\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# 2) Fit detector on train split\n",
    "# detector_l2 = fit_whiten_pca(v_ben_ID.numpy(), alpha=0.9)\n",
    "alphas = [.6,0.7,0.9]\n",
    "detectors_l2 = {a: fit_whiten_pca(v_ben_ID.numpy(), alpha=a) for a in alphas}\n",
    "# 3) χ² threshold (squared-L2 residual)\n",
    "chis_df = {a: v_ben_ID.shape[1] - detectors_l2[a][\"P\"].shape[1] for a in alphas}\n",
    "taus_l2 = {a: chi2.ppf(0.95, df=chis_df[a]) ** 0.5 for a in alphas}\n",
    "for a in alphas:\n",
    "    print(f\"χ²-based τ (without val, alpha={a}) = {taus_l2[a]:.4f}\")\n",
    "\n",
    "# 4) Scoring & evaluation\n",
    "def nsp_score_l2(V: np.ndarray, det) -> np.ndarray:\n",
    "    res = residual_vec(V, det)\n",
    "    return np.linalg.norm(res, axis=1)\n",
    "\n",
    "def eval_l2_without_val(v_ben, v_jb, name):\n",
    "    y_true = np.concatenate([np.zeros(len(v_ben)), np.ones(len(v_jb))])\n",
    "    scores = {a: np.concatenate([\n",
    "        nsp_score_l2(v_ben.numpy(), detectors_l2[a]),\n",
    "        nsp_score_l2(v_jb.numpy(), detectors_l2[a]),\n",
    "    ]) for a in detectors_l2}\n",
    "    y_pred = {a: (scores[a] > taus_l2[a]).astype(int) for a in alphas}\n",
    "\n",
    "    au   = {a: roc_auc_score(y_true, scores[a]) for a in alphas}\n",
    "    prec_rec_f1_ = {a: precision_recall_fscore_support(\n",
    "        y_true, y_pred[a], average=\"binary\", pos_label=1, zero_division=0\n",
    "    ) for a in alphas}\n",
    "    accs = {a: accuracy_score(y_true, y_pred[a]) for a in alphas}\n",
    "    tprs = {a: y_pred[a][len(v_ben):].mean() for a in alphas}\n",
    "    fprs = {a: y_pred[a][:len(v_ben)].mean() for a in alphas}\n",
    "    fnrs = {a: (1 - y_pred[a][len(v_ben):]).mean() for a in alphas}\n",
    "    tnrs = {a: (1 - y_pred[a][:len(v_ben)]).mean() for a in alphas}\n",
    "    for a in alphas:\n",
    "        print(f\"alpha={a:>2} , {name:>6} | Acc {accs[a]:.3f} F1 {prec_rec_f1_[a][2]:.3f} AUROC {au[a]:.3f} Prec {prec_rec_f1_[a][0]:.3f}  Rec {prec_rec_f1_[a][1]:.3f} TPR {tprs[a]:.3f} FPR {fprs[a]:.3f} TNR {tnrs[a]:.3f} FNR {fnrs[a]:.3f}\")\n",
    " \n",
    "# Eval on held-out ID val and full OOD\n",
    "eval_l2_without_val(v_ben_ID,  v_jb_ID,  \"ID\")\n",
    "eval_l2_without_val(v_ben_OOD,  v_jb_OOD,  \"OOD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed4796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell: [Detection – χ²-based L2 NSP (without validation split)] ====\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# 2) Fit detector on train split\n",
    "# detector_l2 = fit_whiten_pca(v_ben_ID.numpy(), alpha=0.9)\n",
    "alphas = [.6,0.7,0.9]\n",
    "detectors_l2 = {a: fit_whiten_pca(v_ben_ID.numpy(), alpha=a) for a in alphas}\n",
    "# 3) χ² threshold (squared-L2 residual)\n",
    "chis_df = {a: v_ben_ID.shape[1] - detectors_l2[a][\"P\"].shape[1] for a in alphas}\n",
    "taus_l2 = {a: chi2.ppf(0.95, df=chis_df[a]) ** 0.5 for a in alphas}\n",
    "for a in alphas:\n",
    "    print(f\"χ²-based τ (without val, alpha={a}) = {taus_l2[a]:.4f}\")\n",
    "\n",
    "# 4) Scoring & evaluation\n",
    "def nsp_score_l2(V: np.ndarray, det) -> np.ndarray:\n",
    "    res = residual_vec(V, det)\n",
    "    return np.linalg.norm(res, axis=1)\n",
    "\n",
    "def eval_l2_without_val(v_ben, v_jb, name):\n",
    "    y_true = np.concatenate([np.zeros(len(v_ben)), np.ones(len(v_jb))])\n",
    "    scores = {a: np.concatenate([\n",
    "        nsp_score_l2(v_ben.numpy(), detectors_l2[a]),\n",
    "        nsp_score_l2(v_jb.numpy(), detectors_l2[a]),\n",
    "    ]) for a in detectors_l2}\n",
    "    y_pred = {a: (scores[a] > taus_l2[a]).astype(int) for a in alphas}\n",
    "\n",
    "    au   = {a: roc_auc_score(y_true, scores[a]) for a in alphas}\n",
    "    prec_rec_f1_ = {a: precision_recall_fscore_support(\n",
    "        y_true, y_pred[a], average=\"binary\", pos_label=1, zero_division=0\n",
    "    ) for a in alphas}\n",
    "    accs = {a: accuracy_score(y_true, y_pred[a]) for a in alphas}\n",
    "    tprs = {a: y_pred[a][len(v_ben):].mean() for a in alphas}\n",
    "    fprs = {a: y_pred[a][:len(v_ben)].mean() for a in alphas}\n",
    "    fnrs = {a: (1 - y_pred[a][len(v_ben):]).mean() for a in alphas}\n",
    "    tnrs = {a: (1 - y_pred[a][:len(v_ben)]).mean() for a in alphas}\n",
    "    for a in alphas:\n",
    "        print(f\"alpha={a:>2} , {name:>6} | Acc {accs[a]:.3f} F1 {prec_rec_f1_[a][2]:.3f} AUROC {au[a]:.3f} Prec {prec_rec_f1_[a][0]:.3f}  Rec {prec_rec_f1_[a][1]:.3f} TPR {tprs[a]:.3f} FPR {fprs[a]:.3f} TNR {tnrs[a]:.3f} FNR {fnrs[a]:.3f}\")\n",
    " \n",
    "# Eval on held-out ID val and full OOD\n",
    "eval_l2_without_val(v_ben_ID,  v_jb_ID,  \"ID\")\n",
    "eval_l2_without_val(v_ben_OOD,  v_jb_OOD,  \"OOD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a4d21e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell: [Detection – χ²-based L2 NSP (without validation split)] ====\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# 2) Fit detector on train split\n",
    "# detector_l2 = fit_whiten_pca(v_ben_ID.numpy(), alpha=0.9)\n",
    "alphas = [0.7,0.9]\n",
    "detectors_l2 = {a: fit_whiten_pca(v_ben_ID.numpy(), alpha=a) for a in alphas}\n",
    "# 3) χ² threshold (squared-L2 residual)\n",
    "chis_df = {a: v_ben_ID.shape[1] - detectors_l2[a][\"P\"].shape[1] for a in alphas}\n",
    "taus_l2 = {a: chi2.ppf(0.95, df=chis_df[a]) ** 0.5 for a in alphas}\n",
    "for a in alphas:\n",
    "    print(f\"χ²-based τ (without val, alpha={a}) = {taus_l2[a]:.4f}\")\n",
    "\n",
    "# 4) Scoring & evaluation\n",
    "def nsp_score_l2(V: np.ndarray, det) -> np.ndarray:\n",
    "    res = residual_vec(V, det)\n",
    "    return np.linalg.norm(res, axis=1)\n",
    "\n",
    "def eval_l2_without_val(v_ben, v_jb, name):\n",
    "    y_true = np.concatenate([np.zeros(len(v_ben)), np.ones(len(v_jb))])\n",
    "    scores = {a: np.concatenate([\n",
    "        nsp_score_l2(v_ben.numpy(), detectors_l2[a]),\n",
    "        nsp_score_l2(v_jb.numpy(), detectors_l2[a]),\n",
    "    ]) for a in detectors_l2}\n",
    "    y_pred = {a: (scores[a] > taus_l2[a]).astype(int) for a in alphas}\n",
    "\n",
    "    au   = {a: roc_auc_score(y_true, scores[a]) for a in alphas}\n",
    "    prec_rec_f1_ = {a: precision_recall_fscore_support(\n",
    "        y_true, y_pred[a], average=\"binary\", pos_label=1, zero_division=0\n",
    "    ) for a in alphas}\n",
    "    accs = {a: accuracy_score(y_true, y_pred[a]) for a in alphas}\n",
    "    tprs = {a: y_pred[a][len(v_ben):].mean() for a in alphas}\n",
    "    fprs = {a: y_pred[a][:len(v_ben)].mean() for a in alphas}\n",
    "    tnrs = {a: (1 - y_pred[a][len(v_ben):]).mean() for a in alphas}\n",
    "    fnrs = {a: (1 - y_pred[a][:len(v_ben)]).mean() for a in alphas}\n",
    "    for a in alphas:\n",
    "        print(f\"alpha={a:>2} , {name:>6} | Acc {accs[a]:.3f} F1 {prec_rec_f1_[a][2]:.3f} AUROC {au[a]:.3f} Prec {prec_rec_f1_[a][0]:.3f}  Rec {prec_rec_f1_[a][1]:.3f} TPR {tprs[a]:.3f} FPR {fprs[a]:.3f} TNR {tnrs[a]:.3f} FNR {fnrs[a]:.3f}\")\n",
    "\n",
    "# Eval on held-out ID val and full OOD\n",
    "eval_l2_without_val(v_ben_ID,  v_jb_ID,  \"ID\")\n",
    "eval_l2_without_val(v_ben_OOD,  v_jb_OOD,  \"OOD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da0901dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell: [Detection – χ²-based L2 NSP (without validation split)] ====\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# 2) Fit detector on train split\n",
    "# detector_l2 = fit_whiten_pca(v_ben_ID.numpy(), alpha=0.9)\n",
    "alphas = [0.7,0.9]\n",
    "detectors_l2 = {a: fit_whiten_pca(v_ben_ID.numpy(), alpha=a) for a in alphas}\n",
    "# 3) χ² threshold (squared-L2 residual)\n",
    "chis_df = {a: v_ben_ID.shape[1] - detectors_l2[a][\"P\"].shape[1] for a in alphas}\n",
    "taus_l2 = {a: chi2.ppf(0.95, df=chis_df[a]) ** 0.5 for a in alphas}\n",
    "for a in alphas:\n",
    "    print(f\"χ²-based τ (without val, alpha={a}) = {taus_l2[a]:.4f}\")\n",
    "\n",
    "# 4) Scoring & evaluation\n",
    "def nsp_score_l2(V: np.ndarray, det) -> np.ndarray:\n",
    "    res = residual_vec(V, det)\n",
    "    return np.linalg.norm(res, axis=1)\n",
    "\n",
    "def eval_l2_without_val(v_ben, v_jb, name):\n",
    "    y_true = np.concatenate([np.zeros(len(v_ben)), np.ones(len(v_jb))])\n",
    "    scores = {a: np.concatenate([\n",
    "        nsp_score_l2(v_ben.numpy(), detectors_l2[a]),\n",
    "        nsp_score_l2(v_jb.numpy(), detectors_l2[a]),\n",
    "    ]) for a in detectors_l2}\n",
    "    y_pred = {a: (scores[a] > taus_l2[a]).astype(int) for a in alphas}\n",
    "\n",
    "    au   = {a: roc_auc_score(y_true, scores[a]) for a in alphas}\n",
    "    prec_rec_f1_ = {a: precision_recall_fscore_support(\n",
    "        y_true, y_pred[a], average=\"binary\", pos_label=1, zero_division=0\n",
    "    ) for a in alphas}\n",
    "    accs = {a: accuracy_score(y_true, y_pred[a]) for a in alphas}\n",
    "    tprs = {a: y_pred[a][len(v_ben):].mean() for a in alphas}\n",
    "    fprs = {a: y_pred[a][:len(v_ben)].mean() for a in alphas}\n",
    "    tnrs = {a: (1 - y_pred[a][len(v_ben):]).mean() for a in alphas}\n",
    "    fnrs = {a: (1 - y_pred[a][:len(v_ben)]).mean() for a in alphas}\n",
    "    for a in alphas:\n",
    "        print(f\"alpha={a:>2} , {name:>6} | Acc {accs[a]:.3f} F1 {prec_rec_f1_[a][2]:.3f} AUROC {au[a]:.3f} Prec {prec_rec_f1_[a][0]:.3f}  Rec {prec_rec_f1_[a][1]:.3f} TPR {tprs[a]:.3f} FPR {fprs[a]:.3f} TNR {tnrs[a]:.3f} FNR {fnrs[a]:.3f}\")\n",
    "\n",
    "# Eval on held-out ID val and full OOD\n",
    "eval_l2_without_val(v_ben_ID,  v_jb_ID,  \"ID\")\n",
    "eval_l2_without_val(v_ben_OOD,  v_jb_OOD,  \"OOD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d7db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell: [Detection – χ²-based L2 NSP (without validation split)] ====\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# 2) Fit detector on train split\n",
    "# detector_l2 = fit_whiten_pca(v_ben_ID.numpy(), alpha=0.9)\n",
    "alphas = [0.7,0.9]\n",
    "detectors_l2 = {a: fit_whiten_pca(v_ben_ID.numpy(), alpha=a) for a in alphas}\n",
    "# 3) χ² threshold (squared-L2 residual)\n",
    "chis_df = {a: v_ben_ID.shape[1] - detectors_l2[a][\"P\"].shape[1] for a in alphas}\n",
    "taus_l2 = {a: chi2.ppf(0.95, df=chis_df[a]) ** 0.5 for a in alphas}\n",
    "for a in alphas:\n",
    "    print(f\"χ²-based τ (without val, alpha={a}) = {taus_l2[a]:.4f}\")\n",
    "\n",
    "# 4) Scoring & evaluation\n",
    "def nsp_score_l2(V: np.ndarray, det) -> np.ndarray:\n",
    "    res = residual_vec(V, det)\n",
    "    return np.linalg.norm(res, axis=1)\n",
    "\n",
    "def eval_l2_without_val(v_ben, v_jb, name):\n",
    "    y_true = np.concatenate([np.zeros(len(v_ben)), np.ones(len(v_jb))])\n",
    "    scores = {a: np.concatenate([\n",
    "        nsp_score_l2(v_ben.numpy(), detectors_l2[a]),\n",
    "        nsp_score_l2(v_jb.numpy(), detectors_l2[a]),\n",
    "    ]) for a in detectors_l2}\n",
    "    y_pred = {a: (scores[a] > taus_l2[a]).astype(int) for a in alphas}\n",
    "\n",
    "    au   = {a: roc_auc_score(y_true, scores[a]) for a in alphas}\n",
    "    prec_rec_f1_ = {a: precision_recall_fscore_support(\n",
    "        y_true, y_pred[a], average=\"binary\", pos_label=1, zero_division=0\n",
    "    ) for a in alphas}\n",
    "    accs = {a: accuracy_score(y_true, y_pred[a]) for a in alphas}\n",
    "    tprs = {a: y_pred[a][len(v_ben):].mean() for a in alphas}\n",
    "    fprs = {a: y_pred[a][:len(v_ben)].mean() for a in alphas}\n",
    "    tnrs = {a: (1 - y_pred[a][len(v_ben):]).mean() for a in alphas}\n",
    "    fnrs = {a: (1 - y_pred[a][:len(v_ben)]).mean() for a in alphas}\n",
    "    for a in alphas:\n",
    "        print(f\"alpha={a:>2} , {name:>6} | Acc {accs[a]:.3f} F1 {prec_rec_f1_[a][2]:.3f} AUROC {au[a]:.3f} Prec {prec_rec_f1_[a][0]:.3f}  Rec {prec_rec_f1_[a][1]:.3f} TPR {tprs[a]:.3f} FPR {fprs[a]:.3f} TNR {tnrs[a]:.3f} FNR {fnrs[a]:.3f}\")\n",
    "\n",
    "# Eval on held-out ID val and full OOD\n",
    "eval_l2_without_val(v_ben_ID,  v_jb_ID,  \"ID\")\n",
    "eval_l2_without_val(v_ben_OOD,  v_jb_OOD,  \"OOD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f61a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell: [Detection – χ²-based L2 NSP (without validation split)] ====\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# 2) Fit detector on train split\n",
    "# detector_l2 = fit_whiten_pca(v_ben_ID.numpy(), alpha=0.9)\n",
    "alphas = [0.7,0.9]\n",
    "detectors_l2 = {a: fit_whiten_pca(v_ben_ID.numpy(), alpha=a) for a in alphas}\n",
    "# 3) χ² threshold (squared-L2 residual)\n",
    "chis_df = {a: v_ben_ID.shape[1] - detectors_l2[a][\"P\"].shape[1] for a in alphas}\n",
    "taus_l2 = {a: chi2.ppf(0.95, df=chis_df[a]) ** 0.5 for a in alphas}\n",
    "for a in alphas:\n",
    "    print(f\"χ²-based τ (without val, alpha={a}) = {taus_l2[a]:.4f}\")\n",
    "\n",
    "# 4) Scoring & evaluation\n",
    "def nsp_score_l2(V: np.ndarray, det) -> np.ndarray:\n",
    "    res = residual_vec(V, det)\n",
    "    return np.linalg.norm(res, axis=1)\n",
    "\n",
    "def eval_l2_without_val(v_ben, v_jb, name):\n",
    "    y_true = np.concatenate([np.zeros(len(v_ben)), np.ones(len(v_jb))])\n",
    "    scores = {a: np.concatenate([\n",
    "        nsp_score_l2(v_ben.numpy(), detectors_l2[a]),\n",
    "        nsp_score_l2(v_jb.numpy(), detectors_l2[a]),\n",
    "    ]) for a in detectors_l2}\n",
    "    y_pred = {a: (scores[a] > taus_l2[a]).astype(int) for a in alphas}\n",
    "\n",
    "    au   = {a: roc_auc_score(y_true, scores[a]) for a in alphas}\n",
    "    prec_rec_f1_ = {a: precision_recall_fscore_support(\n",
    "        y_true, y_pred[a], average=\"binary\", pos_label=1, zero_division=0\n",
    "    ) for a in alphas}\n",
    "    accs = {a: accuracy_score(y_true, y_pred[a]) for a in alphas}\n",
    "    tprs = {a: y_pred[a][len(v_ben):].mean() for a in alphas}\n",
    "    fprs = {a: y_pred[a][:len(v_ben)].mean() for a in alphas}\n",
    "    tnrs = {a: (1 - y_pred[a][len(v_ben):]).mean() for a in alphas}\n",
    "    fnrs = {a: (1 - y_pred[a][:len(v_ben)]).mean() for a in alphas}\n",
    "    for a in alphas:\n",
    "        print(f\"alpha={a:>2} , {name:>6} | Acc {accs[a]:.3f} F1 {prec_rec_f1_[a][2]:.3f} AUROC {au[a]:.3f} Prec {prec_rec_f1_[a][0]:.3f}  Rec {prec_rec_f1_[a][1]:.3f} TPR {tprs[a]:.3f} FPR {fprs[a]:.3f} TNR {tnrs[a]:.3f} FNR {fnrs[a]:.3f}\")\n",
    "\n",
    "# Eval on held-out ID val and full OOD\n",
    "eval_l2_without_val(v_ben_ID,  v_jb_ID,  \"ID\")\n",
    "eval_l2_without_val(v_ben_OOD,  v_jb_OOD,  \"OOD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mean L2 distance between benign vs jailbreak framing vectors\n",
    "# def mean_pairwise_dist(A, B):        # A, B are torch tensors\n",
    "#     return torch.cdist(A, B).mean().item()\n",
    "\n",
    "# # print(\"random   Δ_ben−jb:\", mean_pairwise_dist(vf_ben_ID_rand, vf_jb_ID_rand))\n",
    "# print(\"trained  Δ_ben−jb:\", mean_pairwise_dist(vf_ben_ID,   vf_jb_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bd0563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mean L2 distance between benign vs jailbreak framing vectors\n",
    "# def mean_pairwise_dist(A, B):        # A, B are torch tensors\n",
    "#     return torch.cdist(A, B).mean().item()\n",
    "\n",
    "# # print(\"random   Δ_ben−jb:\", mean_pairwise_dist(vf_ben_ID_rand, vf_jb_ID_rand))\n",
    "# print(\"trained  Δ_ben−jb:\", mean_pairwise_dist(v_ben_ID,   v_jb_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec6555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mean L2 distance between benign vs jailbreak framing vectors\n",
    "# def mean_pairwise_dist(A, B):        # A, B are torch tensors\n",
    "#     return torch.cdist(A, B).mean().item()\n",
    "\n",
    "# print(\"random   Δ_ben−jb:\", mean_pairwise_dist(vf_ben_ID, vf_jb_ID))\n",
    "# # print(\"trained  Δ_ben−jb:\", mean_pairwise_dist(vf_ben_ID,   vf_jb_ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd14280",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d296867",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Venv",
   "language": "python",
   "name": "mykernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
